{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDKEGunIbMro"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31417,
     "status": "ok",
     "timestamp": 1583213923083,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "_Dky7_ZTBeYA",
    "outputId": "23b32861-4180-4bd3-b4ce-0f9265ca39d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIlPWGIwBVFv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib.font_manager import FontProperties\n",
    "prop = FontProperties()\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "prop.set_file('/content/drive/My Drive/Bengali Grapheme/data/kalpurush.ttf')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9477,
     "status": "ok",
     "timestamp": 1583213934625,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "Ya5LLb04BVGB",
    "outputId": "58e70ef1-a236-4224-b8c9-f3b051695da3"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from IPython.display import SVG\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzNNF2REBVGT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Lambda, add\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmFXI_CbBVGe"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_parquet('/content/drive/My Drive/Bengali Grapheme/data/train_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTxxWHQyB__f"
   },
   "outputs": [],
   "source": [
    "X = dataframe.drop(['image_id','grapheme_root','vowel_diacritic','consonant_diacritic'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FyxsyhtvCbJr"
   },
   "outputs": [],
   "source": [
    "y = dataframe[['grapheme_root','vowel_diacritic','consonant_diacritic']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4hK5DFkCl4O"
   },
   "outputs": [],
   "source": [
    "del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QjsLZmfJNub"
   },
   "outputs": [],
   "source": [
    "X = X/255\n",
    "histories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJgpHrXlBVG7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNpYvMdDBVG_"
   },
   "source": [
    "## Generators, Augmentation and Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYAKbKEqb8s5"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pBHi0seBVHA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usiMH_r-PnNW"
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSeL7gd64h_a"
   },
   "outputs": [],
   "source": [
    "def apply_augmentation(image):\n",
    "  augmentation_pipeline = A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% of images\n",
    "                    A.RandomContrast(), # apply random contrast\n",
    "                    A.RandomGamma(), # apply random gamma\n",
    "                    A.RandomBrightness(), # apply random brightness\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% images\n",
    "                    A.ElasticTransform(alpha = 120,sigma = 120 * 0.05,alpha_affine = 120 * 0.03),\n",
    "                    A.GridDistortion(),\n",
    "                    A.OpticalDistortion(distort_limit = 2,shift_limit = 0.5),\n",
    "                    #A.ShiftScaleRotate()\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "        ],\n",
    "        p = 0.5\n",
    "    )\n",
    "  images_aug = augmentation_pipeline(image = image)['image']\n",
    "  return images_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGAQwsdQ5eFD"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqCXg9kd5yap"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
    " \n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.base_m = base_m\n",
    "        self.max_m = max_m\n",
    "        self.cyclical_momentum = cyclical_momentum\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.clr_iterations = 0.\n",
    "        self.cm_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
    "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
    "    \n",
    "    def cm(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            \n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
    "            return self.max_m\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "            \n",
    "        if self.cyclical_momentum == True:\n",
    "            if self.clr_iterations == 0:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            else:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            \n",
    "            \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            K.set_value(self.model.optimizer.momentum, self.cm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlPOckjq5hY4"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(chkpoint_name, log_name, batch_size,epochs):\n",
    "\n",
    "  checkpoint_cb = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/Bengali Grapheme/checkpoints/'\n",
    "  +chkpoint_name,save_best_only=True)\n",
    "  \n",
    "  early_stopping_cb = keras.callbacks.EarlyStopping(monitor='root_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "  root_reduceLR_cb = keras.callbacks.ReduceLROnPlateau(monitor='root_loss', factor=0.2,\n",
    "                                                  patience=5, min_lr=0.0001)\n",
    "  vowel_reduceLR_cb = keras.callbacks.ReduceLROnPlateau(monitor='vowel_loss', factor=0.2,\n",
    "                                                  patience=5, min_lr=0.0001)\n",
    "  consonant_reduceLR_cb = keras.callbacks.ReduceLROnPlateau(monitor='consonant_loss', factor=0.2,\n",
    "                                                  patience=5, min_lr=0.0001)\n",
    "  \n",
    "  root_logdir = '/content/drive/My Drive/Bengali Grapheme/logs'\n",
    "    \n",
    "  batch_size = batch_size\n",
    "  epochs = epochs\n",
    "  max_lr = 0.5\n",
    "  base_lr = max_lr/10\n",
    "  max_m = 0.98\n",
    "  base_m = 0.85\n",
    "\n",
    "  cyclical_momentum = True\n",
    "  augment = True\n",
    "  cycles = 2.35\n",
    "    \n",
    "  def get_run_logdir(name):\n",
    "    import time\n",
    "    run_id = time.strftime(name)\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "\n",
    "  run_logdir = get_run_logdir(log_name+'-run_%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "  tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "  iterations = round(X_train.shape[0]/batch_size*epochs)\n",
    "  iterations = list(range(0,iterations+1))\n",
    "  step_size = len(iterations)/(cycles)\n",
    "  cyclicLR_cb =  CyclicLR(base_lr=base_lr,\n",
    "                            max_lr=max_lr,\n",
    "                            step_size=step_size,\n",
    "                            max_m=max_m,\n",
    "                            base_m=base_m,\n",
    "                            cyclical_momentum=cyclical_momentum)\n",
    "    \n",
    "  return [checkpoint_cb, \n",
    "          root_reduceLR_cb, vowel_reduceLR_cb, consonant_reduceLR_cb,\n",
    "          tensorboard_cb, early_stopping_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3wVE-JbbG3z"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqSbk5egbIkR"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCv-APIsbdvH"
   },
   "source": [
    "### Define Inception, Residual, Inceptual, DenseNet and FractalNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELCx3-nYXv5o"
   },
   "outputs": [],
   "source": [
    "def residual_module(layer_in, n_filters):\n",
    "  merge_input = layer_in\n",
    "  if layer_in.shape[-1] != n_filters:\n",
    "    merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "    merge_input = BatchNormalization()(merge_input)\n",
    "    \n",
    "  conv1 = Conv2D(n_filters, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "  bn1 = BatchNormalization()(conv1)\n",
    "  \n",
    "  # conv2\n",
    "  conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(bn1)\n",
    "  bn2 = BatchNormalization()(conv2)\n",
    "\t\n",
    "  # add filters, assumes filters/channels last\n",
    "  layer_out = add([bn2, merge_input])\n",
    "  # activation function\n",
    "  layer_out = Activation('relu')(layer_out)\n",
    "  return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15epT-kxDr4N"
   },
   "outputs": [],
   "source": [
    "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
    "\t# 1x1 conv\n",
    "\tconv1 = Conv2D(f1, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "\t# 3x3 conv\n",
    "\tconv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "\tconv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(conv3)\n",
    "\t# 5x5 conv\n",
    "\tconv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "\tconv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu', kernel_initializer='he_normal')(conv5)\n",
    "\t# 3x3 max pooling\n",
    "\tpool = MaxPool2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "\tpool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n",
    "\t# concatenate filters, assumes filters/channels last\n",
    "\tlayer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "\treturn layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVRzz6-U-Ac5"
   },
   "outputs": [],
   "source": [
    "def inceptual_module(layer_in, n_filters, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
    "\n",
    "  merge_input = layer_in\n",
    "  if layer_in.shape[-1] != n_filters:\n",
    "    merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "    merge_input = BatchNormalization()(merge_input)\n",
    "    \n",
    "  inception1 = inception_module(merge_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out)\n",
    "  bn1 = BatchNormalization()(inception1)\n",
    "  \n",
    "  # conv2\n",
    "  conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(bn1)\n",
    "  bn2 = BatchNormalization()(conv2)\n",
    "\t\n",
    "  # add filters, assumes filters/channels last\n",
    "  layer_out = add([bn2, merge_input])\n",
    "  # activation function\n",
    "  layer_out = Activation('relu')(layer_out)\n",
    "  return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5e9ufp_Brmn"
   },
   "outputs": [],
   "source": [
    "def conv_layer(conv_x, filters):\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n",
    "    conv_x = Dropout(0.2)(conv_x)\n",
    "\n",
    "    return conv_x\n",
    "\n",
    "def dense_block(block_x, filters, growth_rate, layers_in_block):\n",
    "    for i in range(layers_in_block):\n",
    "        each_layer = conv_layer(block_x, growth_rate)\n",
    "        block_x = concatenate([block_x, each_layer], axis=-1)\n",
    "        filters += growth_rate\n",
    "\n",
    "    return block_x, filters\n",
    "\n",
    "def transition_block(trans_x, tran_filters):\n",
    "    trans_x = BatchNormalization()(trans_x)\n",
    "    trans_x = Activation('relu')(trans_x)\n",
    "    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n",
    "    trans_x = AvgPool2D((2, 2), strides=(2, 2))(trans_x)\n",
    "\n",
    "    return trans_x, tran_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5W1UwS2JKke"
   },
   "outputs": [],
   "source": [
    "def tensorflow_categorical(count, seed):\n",
    "  assert count > 0\n",
    "  arr = [1.] + [.0 for _ in range(count-1)]\n",
    "  return tf.random.shuffle(arr, seed)\n",
    "\n",
    "def rand_one_in_array(count, seed=42):\n",
    "  return tensorflow_categorical(count=count, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3m9J86ixae4N"
   },
   "outputs": [],
   "source": [
    "class JoinLayer(keras.layers.Layer):\n",
    "  def __init__(self, drop_p, is_global, global_path, force_path, **kwargs):\n",
    "    #print \"init\"\n",
    "    self.p = 1. - drop_p\n",
    "    self.is_global = is_global\n",
    "    self.global_path = global_path\n",
    "    self.uses_learning_phase = True\n",
    "    self.force_path = force_path\n",
    "    super(JoinLayer, self).__init__(**kwargs)\n",
    "  \n",
    "  def build(self, input_shape):\n",
    "    #print(\"build\")\n",
    "    self.average_shape = list(input_shape[0])[1:]\n",
    "    \n",
    "  def _random_arr(self, count, p):\n",
    "    return K.random_binomial((count,), p=p)\n",
    "\n",
    "  def _arr_with_one(self, count):\n",
    "    return rand_one_in_array(count=count)\n",
    "\n",
    "  def _gen_local_drops(self, count, p):\n",
    "    # Create a local droppath with at least one path\n",
    "    arr = self._random_arr(count, p)\n",
    "    drops = K.switch(K.any(arr),arr,self._arr_with_one(count))\n",
    "    return drops\n",
    "\n",
    "  def _gen_global_path(self, count):\n",
    "    return self.global_path[:count]\n",
    "\n",
    "  def _drop_path(self, inputs):\n",
    "    count = len(inputs)\n",
    "    drops = K.switch(self.is_global,self._gen_global_path(count),self._gen_local_drops(count, self.p))\n",
    "    \n",
    "    ave = K.zeros(shape=self.average_shape)\n",
    "    for i in range(0, count):\n",
    "      ave = ave + (inputs[i] * drops[i])\n",
    "    sum = K.sum(drops)\n",
    "    # Check that the sum is not 0 (global droppath can make it 0) to avoid divByZero\n",
    "    ave = K.switch(K.not_equal(sum, 0.),ave/sum,ave)\n",
    "    \n",
    "    return ave\n",
    "\n",
    "  def _ave(self, inputs):\n",
    "    ave = inputs[0]\n",
    "    for input in inputs[1:]:\n",
    "      ave = ave + input\n",
    "    ave = ave/len(inputs)\n",
    "    return ave\n",
    "\n",
    "  def call(self, inputs, mask=None):\n",
    "    #print(\"call\")\n",
    "    if self.force_path:\n",
    "      output = self._drop_path(inputs)\n",
    "    else:\n",
    "      output = K.in_train_phase(self._drop_path(inputs), self._ave(inputs))\n",
    "    return output\n",
    "\n",
    "  def get_output_shape_for(self, input_shape):\n",
    "    #print(\"get_output_shape_for\", input_shape)\n",
    "    return input_shape[0]\n",
    "  \n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {**base_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51HLTUU_ao-m"
   },
   "outputs": [],
   "source": [
    "class JoinLayerGen:\n",
    "  \n",
    "  def __init__(self, width, global_p=0.5, deepest=False):\n",
    "    self.global_p = global_p\n",
    "    self.width = width\n",
    "    self.switch_seed = np.random.randint(1, 10e6)\n",
    "    self.path_seed = np.random.randint(1, 10e6)\n",
    "    self.deepest = deepest\n",
    "    if deepest:\n",
    "      self.is_global = K.variable(1.)\n",
    "      self.path_array = K.variable([1.] + [.0 for _ in range(width-1)])\n",
    "    else:\n",
    "      self.is_global = self._build_global_switch()\n",
    "      self.path_array = self._build_global_path_arr()\n",
    "\n",
    "  def _build_global_path_arr(self):\n",
    "    # The path the block will take when using global droppath\n",
    "    return rand_one_in_array(seed=self.path_seed, count=self.width)\n",
    "\n",
    "  def _build_global_switch(self):\n",
    "    # A randomly sampled tensor that will signal if the batch\n",
    "    # should use global or local droppath\n",
    "    return K.equal(K.random_binomial((), p=self.global_p, seed=self.switch_seed), 1.)\n",
    "\n",
    "  def get_join_layer(self, drop_p):\n",
    "    global_switch = self.is_global\n",
    "    global_path = self.path_array\n",
    "    return JoinLayer(drop_p=drop_p, is_global=global_switch, global_path=global_path, force_path=self.deepest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0nlQAGobiLL"
   },
   "outputs": [],
   "source": [
    "def fractal_conv(filters, nb_row, nb_col, dropout=None):\n",
    "  def f(prev):\n",
    "    conv = prev\n",
    "    conv = Conv2D(filters, kernel_size=(nb_row, nb_col), kernel_initializer='he_normal', padding='same')(conv)\n",
    "    if dropout:e\n",
    "      conv = Dropout(dropout)(conv)\n",
    "    conv = BatchNormalization(axis=-1)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "  return f\n",
    "\n",
    "def fractal_block(join_gen, c, filters, nb_col, nb_row, drop_p, dropout=None):\n",
    "  def f(z):\n",
    "    columns = [[z] for _ in range(c)]\n",
    "    last_row = 2**(c-1) - 1\n",
    "    for row in range(2**(c-1)):\n",
    "      t_row = []\n",
    "      for col in range(c):\n",
    "        prop = 2**(col)\n",
    "        # Add blocks\n",
    "        if (row+1) % prop == 0:\n",
    "          t_col = columns[col]\n",
    "          t_col.append(fractal_conv(filters=filters,nb_col=nb_col,\n",
    "                                    nb_row=nb_row,\n",
    "                                    dropout=dropout)(t_col[-1]))\n",
    "          t_row.append(col)\n",
    "        # Merge (if needed)\n",
    "      if len(t_row) > 1:\n",
    "        merging = [columns[x][-1] for x in t_row]\n",
    "        merged  = join_gen.get_join_layer(drop_p=drop_p)(merging)\n",
    "        for i in t_row:\n",
    "          columns[i].append(merged)\n",
    "    return columns[0][-1]\n",
    "  return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X15ckDoifHto"
   },
   "outputs": [],
   "source": [
    "def fractal_net(b, c, conv, drop_path, global_p=0.5, dropout=None, deepest=False):\n",
    "  def f(z):\n",
    "    output = z\n",
    "    # Initialize a JoinLayerGen that will be used to derive the\n",
    "    # JoinLayers that share the same global droppath\n",
    "    join_gen = JoinLayerGen(width=c, global_p=global_p, deepest=deepest)\n",
    "    for i in range(b):\n",
    "      (filters, nb_col, nb_row) = conv[i]\n",
    "      dropout_i = dropout[i] if dropout else None\n",
    "      output = fractal_block(join_gen=join_gen,\n",
    "                             c=c, filters=filters,\n",
    "                             nb_col=nb_col,nb_row=nb_row,\n",
    "                             drop_p=drop_path,\n",
    "                             dropout=dropout_i)(output)\n",
    "      output = MaxPool2D(pool_size=(2,2), strides=(2,2))(output)\n",
    "    return output\n",
    "  return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPuNwk2VcFN3"
   },
   "source": [
    "### Alexnet version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "68OLCjtzBVIc"
   },
   "outputs": [],
   "source": [
    "def model_alexnet():\n",
    "    \n",
    "    input_ = Input(shape=(64,64,1))\n",
    "\n",
    "    augmentation = Lambda(apply_augmentation, output_shape=(64,64,1))(input_)\n",
    "\n",
    "    conv1 = Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', activation='relu')(augmentation)\n",
    "    maxpool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv1)\n",
    "    bn1 = BatchNormalization()(maxpool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu')(bn1)\n",
    "    maxpool2 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv2)\n",
    "    bn2 = BatchNormalization()(maxpool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(bn2)\n",
    "    drop1 = Dropout(0.5)(conv3)\n",
    "    conv4 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop1)\n",
    "    drop2 = Dropout(0.5)(conv4)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop2)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense_root_1 = Dense(4096,activation='relu')(flat)\n",
    "    dense_root_2 = Dense(4096,activation='relu')(dense_root_1)\n",
    "    dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "    output_root = Dense(168,activation='softmax',name='root')(dense_root_3)\n",
    "    \n",
    "    dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "    dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "    output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "    dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "    dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "    output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwHEZRzuBVIj"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_alexnet = model_alexnet()\n",
    "#keras.utils.plot_model(model_alexnet, '../results/alexnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEk_f1kQBVIq"
   },
   "outputs": [],
   "source": [
    "model_alexnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = 'Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gy5E61iyBVI5"
   },
   "outputs": [],
   "source": [
    "history_alexnet = model_alexnet.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=30, callbacks=get_callbacks('alexnet.h5','alexnet.h5',32,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Acvvbm5GaR3L"
   },
   "outputs": [],
   "source": [
    "model_alexnet.save('/content/drive/My Drive/Bengali Grapheme/models/alexnet_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAw_dPwHcJ-S"
   },
   "source": [
    "### SayanNet v1 - Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcOlwvoobb1h"
   },
   "outputs": [],
   "source": [
    "def model_sayannet():\n",
    "  \n",
    "  input_ = Input(shape=(64,64,1))\n",
    "\n",
    "  augmentation = Lambda(apply_augmentation)(input_)\n",
    "\n",
    "  residual1 = residual_module(augmentation, 64)\n",
    "  residual2 = residual_module(residual1,128)\n",
    "\n",
    "  flat = GlobalAveragePooling2D()(residual2)\n",
    "\n",
    "  output_root = Dense(168,activation='softmax',name='root')(flat)\n",
    "\n",
    "  output_vowel = Dense(11,activation='softmax',name='vowel')(flat)\n",
    "  \n",
    "  output_consonant = Dense(7,activation='softmax',name='consonant')(flat)\n",
    "    \n",
    "  model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-36ttfIftpM"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "model_sayannet = model_sayannet()\n",
    "keras.utils.plot_model(model_sayannet, '/content/drive/My Drive/Bengali Grapheme/results/sayannet_v1.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0YFRNkCf6Cn"
   },
   "outputs": [],
   "source": [
    "model_sayannet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = 'Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkWg-F6NlO9B"
   },
   "outputs": [],
   "source": [
    "history_sayannet = model_sayannet.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=30, callbacks=get_callbacks('sayannet.h5','sayannet.h5',32,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzAviy-AlWqz"
   },
   "outputs": [],
   "source": [
    "model_sayannet.save('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtCVWcbVo7_M"
   },
   "source": [
    "### SayanNet v2 - Inception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "656Aim1ppAV8"
   },
   "outputs": [],
   "source": [
    "def model_sayannet_v2():\n",
    "  input_ = Input(shape=(64,64,1))\n",
    "  #augmentation = Lambda(apply_augmentation, output_shape=(64,64,1))(input_)\n",
    "\n",
    "  inception1 = inception_module(input_, 64, 96, 128, 16, 32, 32)\n",
    "  # add inception block 1\n",
    "  inception2 = inception_module(inception1, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "  flat = GlobalAveragePooling2D()(inception2)\n",
    "\n",
    "  output_root = Dense(168,activation='softmax',name='root')(flat)\n",
    "\n",
    "  output_vowel = Dense(11,activation='softmax',name='vowel')(flat)\n",
    "  \n",
    "  output_consonant = Dense(7,activation='softmax',name='consonant')(flat)\n",
    "    \n",
    "  model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CZD5tRA6lnG"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "model_sayannet_v2 = model_sayannet_v2()\n",
    "keras.utils.plot_model(model_sayannet_v2, '/content/drive/My Drive/Bengali Grapheme/results/sayannet_v2.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YzYxQF5zo5s"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v2.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = 'Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeRTsnPL6rBL"
   },
   "outputs": [],
   "source": [
    "history_sayannet_v2 = model_sayannet_v2.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=30, callbacks=get_callbacks('sayannet_v2.h5','sayannet_v2.h5',32,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOGFTB2P68x4"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v2.save('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yBeflKG6dIH"
   },
   "source": [
    "### SayanNet v3 - Inceptual (Inception + Residual) Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecOtWRzB6hv7"
   },
   "outputs": [],
   "source": [
    "def model_sayannet_v3():\n",
    "  input_ = Input(shape=(64,64,1))\n",
    "  #augmentation = Lambda(apply_augmentation, output_shape=(64,64,1))(input_)\n",
    "\n",
    "  inception1 = inceptual_module(input_, 64, 64, 96, 128, 16, 32, 32)\n",
    "  inception2 = inceptual_module(inception1, 128, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "  flat = GlobalAveragePooling2D()(inception2)\n",
    "\n",
    "  dense_root_1 = Dense(4096,activation='relu')(flat)\n",
    "  dense_root_2 = Dense(4096,activation='relu')(dense_root_1)\n",
    "  dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "  output_root = Dense(168,activation='softmax',name='root')(dense_root_3)\n",
    "    \n",
    "  dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "  dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "  dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "  output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "  dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "  dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "  dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "  output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "  model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dKH4K6VBlnm"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "model_sayannet_v3 = model_sayannet_v3()\n",
    "keras.utils.plot_model(model_sayannet_v3, '/content/drive/My Drive/Bengali Grapheme/results/sayannet_v3.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xmJ2uKVBeYr"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v3.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = 'Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayVvg2k3BgR-"
   },
   "outputs": [],
   "source": [
    "history_sayannet_v3 = model_sayannet_v3.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=30, callbacks=get_callbacks('sayannet_v3.h5','sayannet_v3.h5',32,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnKrsXODBiVQ"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v3.save('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YNQ5J1YBX1G"
   },
   "source": [
    "### SayanNet v4 - Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ts7quQ9aDnS9"
   },
   "outputs": [],
   "source": [
    "def model_sayannet_v4(filters=24, growth_rate=12, dense_block_size=3, layers_in_block=4):\n",
    "\n",
    "  dense_block_size = 3\n",
    "  layers_in_block = 4\n",
    "\n",
    "  growth_rate = 12\n",
    "  \n",
    "  input_ = Input(shape=(64,64,1))\n",
    "  #augmentation = Lambda(apply_augmentation, output_shape=(64,64,1))(input_)\n",
    "\n",
    "  conv1 = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_)\n",
    "\n",
    "  bn1 = BatchNormalization()(conv1)\n",
    "  act_x = Activation('relu')(bn1)\n",
    "\n",
    "  dense_x = MaxPool2D((3, 3), strides=(2, 2), padding='same')(act_x)\n",
    "  for block in range(dense_block_size - 1):\n",
    "    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "    dense_x, filters = transition_block(dense_x, filters)\n",
    "\n",
    "  dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
    "  bn_l = BatchNormalization()(dense_x)\n",
    "  act_l = Activation('relu')(bn_l)\n",
    "  flat = GlobalAveragePooling2D()(act_l)\n",
    "  \n",
    "  output_root = Dense(168,activation='softmax',name='root')(flat)\n",
    "\n",
    "  output_vowel = Dense(11,activation='softmax',name='vowel')(flat)\n",
    "  \n",
    "  output_consonant = Dense(7,activation='softmax',name='consonant')(flat)\n",
    "    \n",
    "  model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaYV244rFdTD"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "model_sayannet_v4 = model_sayannet_v4()\n",
    "keras.utils.plot_model(model_sayannet_v4, '/content/drive/My Drive/Bengali Grapheme/results/sayannet_v4.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euVmvbaSGA6F"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v4.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8774214,
     "status": "ok",
     "timestamp": 1583222851293,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "ms_rFSWmGG-S",
    "outputId": "66ee1f67-ff4c-4bb5-f36b-675a985f2f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180756 samples, validate on 20084 samples\n",
      "Epoch 1/100\n",
      "    32/180756 [..............................] - ETA: 13:34:25 - loss: 9.8429 - root_loss: 5.2477 - vowel_loss: 2.5267 - consonant_loss: 2.0685 - root_accuracy: 0.0000e+00 - vowel_accuracy: 0.0625 - consonant_accuracy: 0.0625WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116325). Check your callbacks.\n",
      "180756/180756 [==============================] - 99s 548us/sample - loss: 2.6851 - root_loss: 1.8676 - vowel_loss: 0.4508 - consonant_loss: 0.3666 - root_accuracy: 0.5148 - vowel_accuracy: 0.8502 - consonant_accuracy: 0.8765 - val_loss: 1.6695 - val_root_loss: 1.1073 - val_vowel_loss: 0.2413 - val_consonant_loss: 0.3208 - val_root_accuracy: 0.6922 - val_vowel_accuracy: 0.9244 - val_consonant_accuracy: 0.8997\n",
      "Epoch 2/100\n",
      "180756/180756 [==============================] - 90s 498us/sample - loss: 1.2708 - root_loss: 0.8168 - vowel_loss: 0.2372 - consonant_loss: 0.2168 - root_accuracy: 0.7663 - vowel_accuracy: 0.9249 - consonant_accuracy: 0.9302 - val_loss: 1.3396 - val_root_loss: 0.8585 - val_vowel_loss: 0.2592 - val_consonant_loss: 0.2217 - val_root_accuracy: 0.7723 - val_vowel_accuracy: 0.9191 - val_consonant_accuracy: 0.9335\n",
      "Epoch 3/100\n",
      "180756/180756 [==============================] - 90s 496us/sample - loss: 1.0439 - root_loss: 0.6558 - vowel_loss: 0.1998 - consonant_loss: 0.1883 - root_accuracy: 0.8119 - vowel_accuracy: 0.9371 - consonant_accuracy: 0.9402 - val_loss: 0.9639 - val_root_loss: 0.5950 - val_vowel_loss: 0.1745 - val_consonant_loss: 0.1941 - val_root_accuracy: 0.8348 - val_vowel_accuracy: 0.9489 - val_consonant_accuracy: 0.9403\n",
      "Epoch 4/100\n",
      "180756/180756 [==============================] - 87s 481us/sample - loss: 0.9226 - root_loss: 0.5712 - vowel_loss: 0.1792 - consonant_loss: 0.1722 - root_accuracy: 0.8338 - vowel_accuracy: 0.9439 - consonant_accuracy: 0.9453 - val_loss: 0.9031 - val_root_loss: 0.5860 - val_vowel_loss: 0.1601 - val_consonant_loss: 0.1569 - val_root_accuracy: 0.8392 - val_vowel_accuracy: 0.9536 - val_consonant_accuracy: 0.9520\n",
      "Epoch 5/100\n",
      "180756/180756 [==============================] - 87s 479us/sample - loss: 0.8499 - root_loss: 0.5224 - vowel_loss: 0.1671 - consonant_loss: 0.1603 - root_accuracy: 0.8476 - vowel_accuracy: 0.9484 - consonant_accuracy: 0.9491 - val_loss: 0.9510 - val_root_loss: 0.5315 - val_vowel_loss: 0.2022 - val_consonant_loss: 0.2169 - val_root_accuracy: 0.8559 - val_vowel_accuracy: 0.9448 - val_consonant_accuracy: 0.9374\n",
      "Epoch 6/100\n",
      "180756/180756 [==============================] - 87s 483us/sample - loss: 0.7959 - root_loss: 0.4856 - vowel_loss: 0.1577 - consonant_loss: 0.1526 - root_accuracy: 0.8573 - vowel_accuracy: 0.9515 - consonant_accuracy: 0.9518 - val_loss: 0.8552 - val_root_loss: 0.5362 - val_vowel_loss: 0.1470 - val_consonant_loss: 0.1717 - val_root_accuracy: 0.8569 - val_vowel_accuracy: 0.9583 - val_consonant_accuracy: 0.9501\n",
      "Epoch 7/100\n",
      "180756/180756 [==============================] - 87s 482us/sample - loss: 0.7567 - root_loss: 0.4597 - vowel_loss: 0.1502 - consonant_loss: 0.1469 - root_accuracy: 0.8649 - vowel_accuracy: 0.9541 - consonant_accuracy: 0.9536 - val_loss: 0.7654 - val_root_loss: 0.4777 - val_vowel_loss: 0.1448 - val_consonant_loss: 0.1427 - val_root_accuracy: 0.8695 - val_vowel_accuracy: 0.9592 - val_consonant_accuracy: 0.9591\n",
      "Epoch 8/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.7282 - root_loss: 0.4388 - vowel_loss: 0.1468 - consonant_loss: 0.1425 - root_accuracy: 0.8706 - vowel_accuracy: 0.9551 - consonant_accuracy: 0.9552 - val_loss: 0.7734 - val_root_loss: 0.4893 - val_vowel_loss: 0.1522 - val_consonant_loss: 0.1315 - val_root_accuracy: 0.8659 - val_vowel_accuracy: 0.9558 - val_consonant_accuracy: 0.9618\n",
      "Epoch 9/100\n",
      "180756/180756 [==============================] - 87s 479us/sample - loss: 0.7049 - root_loss: 0.4242 - vowel_loss: 0.1417 - consonant_loss: 0.1390 - root_accuracy: 0.8749 - vowel_accuracy: 0.9567 - consonant_accuracy: 0.9558 - val_loss: 0.7907 - val_root_loss: 0.4851 - val_vowel_loss: 0.1306 - val_consonant_loss: 0.1747 - val_root_accuracy: 0.8719 - val_vowel_accuracy: 0.9626 - val_consonant_accuracy: 0.9497\n",
      "Epoch 10/100\n",
      "180756/180756 [==============================] - 87s 482us/sample - loss: 0.6812 - root_loss: 0.4075 - vowel_loss: 0.1387 - consonant_loss: 0.1349 - root_accuracy: 0.8801 - vowel_accuracy: 0.9575 - consonant_accuracy: 0.9565 - val_loss: 0.7663 - val_root_loss: 0.4704 - val_vowel_loss: 0.1514 - val_consonant_loss: 0.1442 - val_root_accuracy: 0.8716 - val_vowel_accuracy: 0.9596 - val_consonant_accuracy: 0.9593\n",
      "Epoch 11/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.6626 - root_loss: 0.3968 - vowel_loss: 0.1348 - consonant_loss: 0.1310 - root_accuracy: 0.8826 - vowel_accuracy: 0.9588 - consonant_accuracy: 0.9585 - val_loss: 0.7658 - val_root_loss: 0.4887 - val_vowel_loss: 0.1499 - val_consonant_loss: 0.1270 - val_root_accuracy: 0.8705 - val_vowel_accuracy: 0.9571 - val_consonant_accuracy: 0.9636\n",
      "Epoch 12/100\n",
      "180756/180756 [==============================] - 87s 480us/sample - loss: 0.6449 - root_loss: 0.3857 - vowel_loss: 0.1309 - consonant_loss: 0.1284 - root_accuracy: 0.8857 - vowel_accuracy: 0.9599 - consonant_accuracy: 0.9596 - val_loss: 0.6932 - val_root_loss: 0.4311 - val_vowel_loss: 0.1298 - val_consonant_loss: 0.1320 - val_root_accuracy: 0.8877 - val_vowel_accuracy: 0.9643 - val_consonant_accuracy: 0.9613\n",
      "Epoch 13/100\n",
      "180756/180756 [==============================] - 87s 480us/sample - loss: 0.6313 - root_loss: 0.3762 - vowel_loss: 0.1288 - consonant_loss: 0.1262 - root_accuracy: 0.8880 - vowel_accuracy: 0.9609 - consonant_accuracy: 0.9599 - val_loss: 0.7166 - val_root_loss: 0.4290 - val_vowel_loss: 0.1315 - val_consonant_loss: 0.1559 - val_root_accuracy: 0.8894 - val_vowel_accuracy: 0.9644 - val_consonant_accuracy: 0.9552\n",
      "Epoch 14/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.6191 - root_loss: 0.3675 - vowel_loss: 0.1267 - consonant_loss: 0.1249 - root_accuracy: 0.8910 - vowel_accuracy: 0.9618 - consonant_accuracy: 0.9607 - val_loss: 0.6598 - val_root_loss: 0.3796 - val_vowel_loss: 0.1308 - val_consonant_loss: 0.1491 - val_root_accuracy: 0.9009 - val_vowel_accuracy: 0.9630 - val_consonant_accuracy: 0.9585\n",
      "Epoch 15/100\n",
      "180756/180756 [==============================] - 87s 479us/sample - loss: 0.6078 - root_loss: 0.3615 - vowel_loss: 0.1242 - consonant_loss: 0.1221 - root_accuracy: 0.8921 - vowel_accuracy: 0.9620 - consonant_accuracy: 0.9613 - val_loss: 0.7044 - val_root_loss: 0.4180 - val_vowel_loss: 0.1548 - val_consonant_loss: 0.1312 - val_root_accuracy: 0.8907 - val_vowel_accuracy: 0.9569 - val_consonant_accuracy: 0.9644\n",
      "Epoch 16/100\n",
      "180756/180756 [==============================] - 86s 477us/sample - loss: 0.5965 - root_loss: 0.3544 - vowel_loss: 0.1216 - consonant_loss: 0.1205 - root_accuracy: 0.8940 - vowel_accuracy: 0.9627 - consonant_accuracy: 0.9620 - val_loss: 0.6690 - val_root_loss: 0.3985 - val_vowel_loss: 0.1315 - val_consonant_loss: 0.1388 - val_root_accuracy: 0.8933 - val_vowel_accuracy: 0.9635 - val_consonant_accuracy: 0.9608\n",
      "Epoch 17/100\n",
      "180756/180756 [==============================] - 88s 486us/sample - loss: 0.5882 - root_loss: 0.3484 - vowel_loss: 0.1216 - consonant_loss: 0.1183 - root_accuracy: 0.8955 - vowel_accuracy: 0.9628 - consonant_accuracy: 0.9630 - val_loss: 0.6597 - val_root_loss: 0.4146 - val_vowel_loss: 0.1211 - val_consonant_loss: 0.1237 - val_root_accuracy: 0.8913 - val_vowel_accuracy: 0.9666 - val_consonant_accuracy: 0.9657\n",
      "Epoch 18/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.5836 - root_loss: 0.3446 - vowel_loss: 0.1203 - consonant_loss: 0.1187 - root_accuracy: 0.8965 - vowel_accuracy: 0.9635 - consonant_accuracy: 0.9626 - val_loss: 0.6914 - val_root_loss: 0.4205 - val_vowel_loss: 0.1257 - val_consonant_loss: 0.1449 - val_root_accuracy: 0.8899 - val_vowel_accuracy: 0.9654 - val_consonant_accuracy: 0.9593\n",
      "Epoch 19/100\n",
      "180756/180756 [==============================] - 87s 483us/sample - loss: 0.5738 - root_loss: 0.3397 - vowel_loss: 0.1182 - consonant_loss: 0.1158 - root_accuracy: 0.8975 - vowel_accuracy: 0.9646 - consonant_accuracy: 0.9637 - val_loss: 0.6393 - val_root_loss: 0.3799 - val_vowel_loss: 0.1270 - val_consonant_loss: 0.1322 - val_root_accuracy: 0.9023 - val_vowel_accuracy: 0.9663 - val_consonant_accuracy: 0.9629\n",
      "Epoch 20/100\n",
      "180756/180756 [==============================] - 89s 490us/sample - loss: 0.5660 - root_loss: 0.3346 - vowel_loss: 0.1168 - consonant_loss: 0.1146 - root_accuracy: 0.8987 - vowel_accuracy: 0.9646 - consonant_accuracy: 0.9644 - val_loss: 0.6439 - val_root_loss: 0.3949 - val_vowel_loss: 0.1212 - val_consonant_loss: 0.1276 - val_root_accuracy: 0.8977 - val_vowel_accuracy: 0.9669 - val_consonant_accuracy: 0.9627\n",
      "Epoch 21/100\n",
      "180756/180756 [==============================] - 90s 497us/sample - loss: 0.5556 - root_loss: 0.3279 - vowel_loss: 0.1148 - consonant_loss: 0.1129 - root_accuracy: 0.9017 - vowel_accuracy: 0.9650 - consonant_accuracy: 0.9641 - val_loss: 0.6375 - val_root_loss: 0.3905 - val_vowel_loss: 0.1275 - val_consonant_loss: 0.1193 - val_root_accuracy: 0.8995 - val_vowel_accuracy: 0.9671 - val_consonant_accuracy: 0.9670\n",
      "Epoch 22/100\n",
      "180756/180756 [==============================] - 89s 494us/sample - loss: 0.5523 - root_loss: 0.3252 - vowel_loss: 0.1140 - consonant_loss: 0.1131 - root_accuracy: 0.9012 - vowel_accuracy: 0.9655 - consonant_accuracy: 0.9640 - val_loss: 0.7070 - val_root_loss: 0.4440 - val_vowel_loss: 0.1337 - val_consonant_loss: 0.1291 - val_root_accuracy: 0.8888 - val_vowel_accuracy: 0.9641 - val_consonant_accuracy: 0.9639\n",
      "Epoch 23/100\n",
      "180756/180756 [==============================] - 89s 493us/sample - loss: 0.5450 - root_loss: 0.3199 - vowel_loss: 0.1137 - consonant_loss: 0.1113 - root_accuracy: 0.9029 - vowel_accuracy: 0.9658 - consonant_accuracy: 0.9652 - val_loss: 0.6551 - val_root_loss: 0.4029 - val_vowel_loss: 0.1363 - val_consonant_loss: 0.1157 - val_root_accuracy: 0.8983 - val_vowel_accuracy: 0.9624 - val_consonant_accuracy: 0.9664\n",
      "Epoch 24/100\n",
      "180756/180756 [==============================] - 89s 491us/sample - loss: 0.5431 - root_loss: 0.3201 - vowel_loss: 0.1121 - consonant_loss: 0.1109 - root_accuracy: 0.9037 - vowel_accuracy: 0.9664 - consonant_accuracy: 0.9647 - val_loss: 0.6591 - val_root_loss: 0.4164 - val_vowel_loss: 0.1214 - val_consonant_loss: 0.1210 - val_root_accuracy: 0.8952 - val_vowel_accuracy: 0.9668 - val_consonant_accuracy: 0.9663\n",
      "Epoch 25/100\n",
      "180756/180756 [==============================] - 87s 481us/sample - loss: 0.5366 - root_loss: 0.3158 - vowel_loss: 0.1126 - consonant_loss: 0.1082 - root_accuracy: 0.9041 - vowel_accuracy: 0.9658 - consonant_accuracy: 0.9658 - val_loss: 0.6477 - val_root_loss: 0.3944 - val_vowel_loss: 0.1265 - val_consonant_loss: 0.1266 - val_root_accuracy: 0.9025 - val_vowel_accuracy: 0.9665 - val_consonant_accuracy: 0.9666\n",
      "Epoch 26/100\n",
      "180756/180756 [==============================] - 87s 483us/sample - loss: 0.5314 - root_loss: 0.3122 - vowel_loss: 0.1103 - consonant_loss: 0.1088 - root_accuracy: 0.9055 - vowel_accuracy: 0.9667 - consonant_accuracy: 0.9659 - val_loss: 0.6729 - val_root_loss: 0.4170 - val_vowel_loss: 0.1302 - val_consonant_loss: 0.1255 - val_root_accuracy: 0.8947 - val_vowel_accuracy: 0.9642 - val_consonant_accuracy: 0.9638\n",
      "Epoch 27/100\n",
      "180756/180756 [==============================] - 88s 484us/sample - loss: 0.5260 - root_loss: 0.3076 - vowel_loss: 0.1106 - consonant_loss: 0.1078 - root_accuracy: 0.9068 - vowel_accuracy: 0.9668 - consonant_accuracy: 0.9663 - val_loss: 0.6424 - val_root_loss: 0.3971 - val_vowel_loss: 0.1240 - val_consonant_loss: 0.1211 - val_root_accuracy: 0.8986 - val_vowel_accuracy: 0.9663 - val_consonant_accuracy: 0.9654\n",
      "Epoch 28/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.5215 - root_loss: 0.3045 - vowel_loss: 0.1093 - consonant_loss: 0.1076 - root_accuracy: 0.9070 - vowel_accuracy: 0.9667 - consonant_accuracy: 0.9663 - val_loss: 0.6321 - val_root_loss: 0.3909 - val_vowel_loss: 0.1221 - val_consonant_loss: 0.1189 - val_root_accuracy: 0.9024 - val_vowel_accuracy: 0.9665 - val_consonant_accuracy: 0.9665\n",
      "Epoch 29/100\n",
      "180756/180756 [==============================] - 87s 484us/sample - loss: 0.5172 - root_loss: 0.3043 - vowel_loss: 0.1076 - consonant_loss: 0.1054 - root_accuracy: 0.9076 - vowel_accuracy: 0.9670 - consonant_accuracy: 0.9667 - val_loss: 0.6289 - val_root_loss: 0.3852 - val_vowel_loss: 0.1207 - val_consonant_loss: 0.1228 - val_root_accuracy: 0.9051 - val_vowel_accuracy: 0.9688 - val_consonant_accuracy: 0.9663\n",
      "Epoch 30/100\n",
      "180756/180756 [==============================] - 86s 477us/sample - loss: 0.5113 - root_loss: 0.2994 - vowel_loss: 0.1072 - consonant_loss: 0.1047 - root_accuracy: 0.9086 - vowel_accuracy: 0.9676 - consonant_accuracy: 0.9669 - val_loss: 0.6382 - val_root_loss: 0.3993 - val_vowel_loss: 0.1191 - val_consonant_loss: 0.1195 - val_root_accuracy: 0.8990 - val_vowel_accuracy: 0.9672 - val_consonant_accuracy: 0.9673\n",
      "Epoch 31/100\n",
      "180756/180756 [==============================] - 87s 482us/sample - loss: 0.5077 - root_loss: 0.2970 - vowel_loss: 0.1064 - consonant_loss: 0.1043 - root_accuracy: 0.9093 - vowel_accuracy: 0.9679 - consonant_accuracy: 0.9671 - val_loss: 0.6577 - val_root_loss: 0.3899 - val_vowel_loss: 0.1233 - val_consonant_loss: 0.1443 - val_root_accuracy: 0.9019 - val_vowel_accuracy: 0.9668 - val_consonant_accuracy: 0.9593\n",
      "Epoch 32/100\n",
      "180756/180756 [==============================] - 86s 477us/sample - loss: 0.5053 - root_loss: 0.2947 - vowel_loss: 0.1051 - consonant_loss: 0.1055 - root_accuracy: 0.9091 - vowel_accuracy: 0.9682 - consonant_accuracy: 0.9668 - val_loss: 0.6354 - val_root_loss: 0.3870 - val_vowel_loss: 0.1204 - val_consonant_loss: 0.1276 - val_root_accuracy: 0.9041 - val_vowel_accuracy: 0.9675 - val_consonant_accuracy: 0.9646\n",
      "Epoch 33/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.4982 - root_loss: 0.2902 - vowel_loss: 0.1045 - consonant_loss: 0.1035 - root_accuracy: 0.9111 - vowel_accuracy: 0.9687 - consonant_accuracy: 0.9672 - val_loss: 0.6615 - val_root_loss: 0.4040 - val_vowel_loss: 0.1257 - val_consonant_loss: 0.1316 - val_root_accuracy: 0.9029 - val_vowel_accuracy: 0.9646 - val_consonant_accuracy: 0.9634\n",
      "Epoch 34/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.4957 - root_loss: 0.2894 - vowel_loss: 0.1040 - consonant_loss: 0.1023 - root_accuracy: 0.9106 - vowel_accuracy: 0.9683 - consonant_accuracy: 0.9677 - val_loss: 0.6423 - val_root_loss: 0.4069 - val_vowel_loss: 0.1106 - val_consonant_loss: 0.1245 - val_root_accuracy: 0.9020 - val_vowel_accuracy: 0.9713 - val_consonant_accuracy: 0.9676\n",
      "Epoch 35/100\n",
      "180756/180756 [==============================] - 88s 486us/sample - loss: 0.4952 - root_loss: 0.2878 - vowel_loss: 0.1048 - consonant_loss: 0.1026 - root_accuracy: 0.9130 - vowel_accuracy: 0.9689 - consonant_accuracy: 0.9677 - val_loss: 0.6427 - val_root_loss: 0.4122 - val_vowel_loss: 0.1190 - val_consonant_loss: 0.1113 - val_root_accuracy: 0.8974 - val_vowel_accuracy: 0.9684 - val_consonant_accuracy: 0.9689\n",
      "Epoch 36/100\n",
      "180756/180756 [==============================] - 89s 490us/sample - loss: 0.4904 - root_loss: 0.2854 - vowel_loss: 0.1038 - consonant_loss: 0.1012 - root_accuracy: 0.9128 - vowel_accuracy: 0.9686 - consonant_accuracy: 0.9682 - val_loss: 0.6545 - val_root_loss: 0.4144 - val_vowel_loss: 0.1266 - val_consonant_loss: 0.1132 - val_root_accuracy: 0.9015 - val_vowel_accuracy: 0.9657 - val_consonant_accuracy: 0.9684\n",
      "Epoch 37/100\n",
      "180756/180756 [==============================] - 87s 483us/sample - loss: 0.4898 - root_loss: 0.2855 - vowel_loss: 0.1035 - consonant_loss: 0.1009 - root_accuracy: 0.9134 - vowel_accuracy: 0.9689 - consonant_accuracy: 0.9682 - val_loss: 0.6747 - val_root_loss: 0.4194 - val_vowel_loss: 0.1217 - val_consonant_loss: 0.1332 - val_root_accuracy: 0.9012 - val_vowel_accuracy: 0.9656 - val_consonant_accuracy: 0.9636\n",
      "Epoch 38/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4847 - root_loss: 0.2819 - vowel_loss: 0.1024 - consonant_loss: 0.1004 - root_accuracy: 0.9135 - vowel_accuracy: 0.9685 - consonant_accuracy: 0.9684 - val_loss: 0.6098 - val_root_loss: 0.3766 - val_vowel_loss: 0.1197 - val_consonant_loss: 0.1132 - val_root_accuracy: 0.9068 - val_vowel_accuracy: 0.9680 - val_consonant_accuracy: 0.9679\n",
      "Epoch 39/100\n",
      "180756/180756 [==============================] - 87s 481us/sample - loss: 0.4813 - root_loss: 0.2789 - vowel_loss: 0.1027 - consonant_loss: 0.0997 - root_accuracy: 0.9145 - vowel_accuracy: 0.9696 - consonant_accuracy: 0.9682 - val_loss: 0.6213 - val_root_loss: 0.3872 - val_vowel_loss: 0.1209 - val_consonant_loss: 0.1129 - val_root_accuracy: 0.9024 - val_vowel_accuracy: 0.9667 - val_consonant_accuracy: 0.9684\n",
      "Epoch 40/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4822 - root_loss: 0.2817 - vowel_loss: 0.1009 - consonant_loss: 0.0996 - root_accuracy: 0.9139 - vowel_accuracy: 0.9694 - consonant_accuracy: 0.9685 - val_loss: 0.6384 - val_root_loss: 0.4035 - val_vowel_loss: 0.1132 - val_consonant_loss: 0.1215 - val_root_accuracy: 0.9014 - val_vowel_accuracy: 0.9692 - val_consonant_accuracy: 0.9671\n",
      "Epoch 41/100\n",
      "180756/180756 [==============================] - 89s 494us/sample - loss: 0.4735 - root_loss: 0.2758 - vowel_loss: 0.0993 - consonant_loss: 0.0985 - root_accuracy: 0.9155 - vowel_accuracy: 0.9701 - consonant_accuracy: 0.9691 - val_loss: 0.6114 - val_root_loss: 0.3748 - val_vowel_loss: 0.1112 - val_consonant_loss: 0.1252 - val_root_accuracy: 0.9076 - val_vowel_accuracy: 0.9720 - val_consonant_accuracy: 0.9649\n",
      "Epoch 42/100\n",
      "180756/180756 [==============================] - 91s 501us/sample - loss: 0.4772 - root_loss: 0.2759 - vowel_loss: 0.1008 - consonant_loss: 0.1004 - root_accuracy: 0.9151 - vowel_accuracy: 0.9695 - consonant_accuracy: 0.9684 - val_loss: 0.6178 - val_root_loss: 0.3860 - val_vowel_loss: 0.1175 - val_consonant_loss: 0.1140 - val_root_accuracy: 0.9047 - val_vowel_accuracy: 0.9678 - val_consonant_accuracy: 0.9691\n",
      "Epoch 43/100\n",
      "180756/180756 [==============================] - 91s 503us/sample - loss: 0.4721 - root_loss: 0.2737 - vowel_loss: 0.0993 - consonant_loss: 0.0991 - root_accuracy: 0.9162 - vowel_accuracy: 0.9701 - consonant_accuracy: 0.9687 - val_loss: 0.6657 - val_root_loss: 0.4242 - val_vowel_loss: 0.1114 - val_consonant_loss: 0.1298 - val_root_accuracy: 0.9007 - val_vowel_accuracy: 0.9715 - val_consonant_accuracy: 0.9647\n",
      "Epoch 44/100\n",
      "180756/180756 [==============================] - 89s 494us/sample - loss: 0.4690 - root_loss: 0.2710 - vowel_loss: 0.1007 - consonant_loss: 0.0974 - root_accuracy: 0.9166 - vowel_accuracy: 0.9693 - consonant_accuracy: 0.9690 - val_loss: 0.6627 - val_root_loss: 0.4190 - val_vowel_loss: 0.1214 - val_consonant_loss: 0.1219 - val_root_accuracy: 0.8990 - val_vowel_accuracy: 0.9662 - val_consonant_accuracy: 0.9668\n",
      "Epoch 45/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4711 - root_loss: 0.2720 - vowel_loss: 0.1007 - consonant_loss: 0.0984 - root_accuracy: 0.9161 - vowel_accuracy: 0.9696 - consonant_accuracy: 0.9688 - val_loss: 0.6414 - val_root_loss: 0.4072 - val_vowel_loss: 0.1193 - val_consonant_loss: 0.1146 - val_root_accuracy: 0.8987 - val_vowel_accuracy: 0.9669 - val_consonant_accuracy: 0.9669\n",
      "Epoch 46/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4619 - root_loss: 0.2685 - vowel_loss: 0.0973 - consonant_loss: 0.0962 - root_accuracy: 0.9170 - vowel_accuracy: 0.9700 - consonant_accuracy: 0.9694 - val_loss: 0.6154 - val_root_loss: 0.3934 - val_vowel_loss: 0.1126 - val_consonant_loss: 0.1091 - val_root_accuracy: 0.9056 - val_vowel_accuracy: 0.9704 - val_consonant_accuracy: 0.9701\n",
      "Epoch 47/100\n",
      "180756/180756 [==============================] - 88s 487us/sample - loss: 0.4627 - root_loss: 0.2673 - vowel_loss: 0.0986 - consonant_loss: 0.0968 - root_accuracy: 0.9174 - vowel_accuracy: 0.9701 - consonant_accuracy: 0.9692 - val_loss: 0.6211 - val_root_loss: 0.3946 - val_vowel_loss: 0.1151 - val_consonant_loss: 0.1111 - val_root_accuracy: 0.9061 - val_vowel_accuracy: 0.9694 - val_consonant_accuracy: 0.9688\n",
      "Epoch 48/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4578 - root_loss: 0.2651 - vowel_loss: 0.0961 - consonant_loss: 0.0965 - root_accuracy: 0.9182 - vowel_accuracy: 0.9709 - consonant_accuracy: 0.9688 - val_loss: 0.6194 - val_root_loss: 0.3864 - val_vowel_loss: 0.1118 - val_consonant_loss: 0.1209 - val_root_accuracy: 0.9079 - val_vowel_accuracy: 0.9710 - val_consonant_accuracy: 0.9697\n",
      "Epoch 49/100\n",
      "180756/180756 [==============================] - 89s 491us/sample - loss: 0.4594 - root_loss: 0.2662 - vowel_loss: 0.0972 - consonant_loss: 0.0960 - root_accuracy: 0.9172 - vowel_accuracy: 0.9707 - consonant_accuracy: 0.9696 - val_loss: 0.6152 - val_root_loss: 0.3878 - val_vowel_loss: 0.1155 - val_consonant_loss: 0.1117 - val_root_accuracy: 0.9054 - val_vowel_accuracy: 0.9698 - val_consonant_accuracy: 0.9704\n",
      "Epoch 50/100\n",
      "180756/180756 [==============================] - 89s 492us/sample - loss: 0.4556 - root_loss: 0.2639 - vowel_loss: 0.0974 - consonant_loss: 0.0943 - root_accuracy: 0.9183 - vowel_accuracy: 0.9707 - consonant_accuracy: 0.9703 - val_loss: 0.6272 - val_root_loss: 0.3816 - val_vowel_loss: 0.1135 - val_consonant_loss: 0.1319 - val_root_accuracy: 0.9087 - val_vowel_accuracy: 0.9700 - val_consonant_accuracy: 0.9642\n",
      "Epoch 51/100\n",
      "180756/180756 [==============================] - 89s 493us/sample - loss: 0.4553 - root_loss: 0.2632 - vowel_loss: 0.0964 - consonant_loss: 0.0957 - root_accuracy: 0.9187 - vowel_accuracy: 0.9708 - consonant_accuracy: 0.9698 - val_loss: 0.6184 - val_root_loss: 0.3884 - val_vowel_loss: 0.1120 - val_consonant_loss: 0.1177 - val_root_accuracy: 0.9086 - val_vowel_accuracy: 0.9717 - val_consonant_accuracy: 0.9675\n",
      "Epoch 52/100\n",
      "180756/180756 [==============================] - 88s 488us/sample - loss: 0.4525 - root_loss: 0.2605 - vowel_loss: 0.0970 - consonant_loss: 0.0950 - root_accuracy: 0.9194 - vowel_accuracy: 0.9706 - consonant_accuracy: 0.9699 - val_loss: 0.6179 - val_root_loss: 0.3931 - val_vowel_loss: 0.1150 - val_consonant_loss: 0.1095 - val_root_accuracy: 0.9057 - val_vowel_accuracy: 0.9706 - val_consonant_accuracy: 0.9704\n",
      "Epoch 53/100\n",
      "180756/180756 [==============================] - 88s 486us/sample - loss: 0.4499 - root_loss: 0.2588 - vowel_loss: 0.0957 - consonant_loss: 0.0954 - root_accuracy: 0.9193 - vowel_accuracy: 0.9711 - consonant_accuracy: 0.9697 - val_loss: 0.6326 - val_root_loss: 0.3974 - val_vowel_loss: 0.1214 - val_consonant_loss: 0.1137 - val_root_accuracy: 0.9041 - val_vowel_accuracy: 0.9689 - val_consonant_accuracy: 0.9692\n",
      "Epoch 54/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4470 - root_loss: 0.2579 - vowel_loss: 0.0952 - consonant_loss: 0.0939 - root_accuracy: 0.9202 - vowel_accuracy: 0.9710 - consonant_accuracy: 0.9706 - val_loss: 0.6166 - val_root_loss: 0.3880 - val_vowel_loss: 0.1120 - val_consonant_loss: 0.1163 - val_root_accuracy: 0.9098 - val_vowel_accuracy: 0.9717 - val_consonant_accuracy: 0.9668\n",
      "Epoch 55/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4492 - root_loss: 0.2596 - vowel_loss: 0.0956 - consonant_loss: 0.0941 - root_accuracy: 0.9198 - vowel_accuracy: 0.9710 - consonant_accuracy: 0.9702 - val_loss: 0.6213 - val_root_loss: 0.3939 - val_vowel_loss: 0.1126 - val_consonant_loss: 0.1145 - val_root_accuracy: 0.9051 - val_vowel_accuracy: 0.9697 - val_consonant_accuracy: 0.9685\n",
      "Epoch 56/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4445 - root_loss: 0.2572 - vowel_loss: 0.0949 - consonant_loss: 0.0924 - root_accuracy: 0.9195 - vowel_accuracy: 0.9711 - consonant_accuracy: 0.9710 - val_loss: 0.6099 - val_root_loss: 0.3863 - val_vowel_loss: 0.1124 - val_consonant_loss: 0.1109 - val_root_accuracy: 0.9080 - val_vowel_accuracy: 0.9714 - val_consonant_accuracy: 0.9699\n",
      "Epoch 57/100\n",
      "180756/180756 [==============================] - 87s 484us/sample - loss: 0.4459 - root_loss: 0.2568 - vowel_loss: 0.0954 - consonant_loss: 0.0937 - root_accuracy: 0.9200 - vowel_accuracy: 0.9713 - consonant_accuracy: 0.9704 - val_loss: 0.6432 - val_root_loss: 0.4010 - val_vowel_loss: 0.1207 - val_consonant_loss: 0.1213 - val_root_accuracy: 0.9036 - val_vowel_accuracy: 0.9698 - val_consonant_accuracy: 0.9668\n",
      "Epoch 58/100\n",
      "180756/180756 [==============================] - 87s 484us/sample - loss: 0.4428 - root_loss: 0.2556 - vowel_loss: 0.0945 - consonant_loss: 0.0927 - root_accuracy: 0.9207 - vowel_accuracy: 0.9716 - consonant_accuracy: 0.9706 - val_loss: 0.6435 - val_root_loss: 0.4034 - val_vowel_loss: 0.1127 - val_consonant_loss: 0.1271 - val_root_accuracy: 0.8992 - val_vowel_accuracy: 0.9704 - val_consonant_accuracy: 0.9658\n",
      "Epoch 59/100\n",
      "180756/180756 [==============================] - 89s 492us/sample - loss: 0.4407 - root_loss: 0.2536 - vowel_loss: 0.0943 - consonant_loss: 0.0928 - root_accuracy: 0.9213 - vowel_accuracy: 0.9716 - consonant_accuracy: 0.9710 - val_loss: 0.6068 - val_root_loss: 0.3827 - val_vowel_loss: 0.1085 - val_consonant_loss: 0.1152 - val_root_accuracy: 0.9072 - val_vowel_accuracy: 0.9714 - val_consonant_accuracy: 0.9688\n",
      "Epoch 60/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4375 - root_loss: 0.2516 - vowel_loss: 0.0930 - consonant_loss: 0.0929 - root_accuracy: 0.9216 - vowel_accuracy: 0.9718 - consonant_accuracy: 0.9706 - val_loss: 0.6447 - val_root_loss: 0.4121 - val_vowel_loss: 0.1138 - val_consonant_loss: 0.1186 - val_root_accuracy: 0.9043 - val_vowel_accuracy: 0.9701 - val_consonant_accuracy: 0.9670\n",
      "Epoch 61/100\n",
      "180756/180756 [==============================] - 88s 488us/sample - loss: 0.4391 - root_loss: 0.2530 - vowel_loss: 0.0942 - consonant_loss: 0.0919 - root_accuracy: 0.9219 - vowel_accuracy: 0.9717 - consonant_accuracy: 0.9713 - val_loss: 0.6572 - val_root_loss: 0.4230 - val_vowel_loss: 0.1184 - val_consonant_loss: 0.1154 - val_root_accuracy: 0.8997 - val_vowel_accuracy: 0.9692 - val_consonant_accuracy: 0.9686\n",
      "Epoch 62/100\n",
      "180756/180756 [==============================] - 89s 494us/sample - loss: 0.4353 - root_loss: 0.2503 - vowel_loss: 0.0933 - consonant_loss: 0.0917 - root_accuracy: 0.9225 - vowel_accuracy: 0.9717 - consonant_accuracy: 0.9713 - val_loss: 0.6293 - val_root_loss: 0.3988 - val_vowel_loss: 0.1088 - val_consonant_loss: 0.1216 - val_root_accuracy: 0.9059 - val_vowel_accuracy: 0.9701 - val_consonant_accuracy: 0.9676\n",
      "Epoch 63/100\n",
      "180756/180756 [==============================] - 90s 498us/sample - loss: 0.4343 - root_loss: 0.2502 - vowel_loss: 0.0919 - consonant_loss: 0.0921 - root_accuracy: 0.9219 - vowel_accuracy: 0.9724 - consonant_accuracy: 0.9706 - val_loss: 0.6121 - val_root_loss: 0.3851 - val_vowel_loss: 0.1117 - val_consonant_loss: 0.1151 - val_root_accuracy: 0.9074 - val_vowel_accuracy: 0.9712 - val_consonant_accuracy: 0.9684\n",
      "Epoch 64/100\n",
      "180756/180756 [==============================] - 89s 495us/sample - loss: 0.4331 - root_loss: 0.2486 - vowel_loss: 0.0932 - consonant_loss: 0.0914 - root_accuracy: 0.9221 - vowel_accuracy: 0.9717 - consonant_accuracy: 0.9711 - val_loss: 0.6248 - val_root_loss: 0.3889 - val_vowel_loss: 0.1250 - val_consonant_loss: 0.1105 - val_root_accuracy: 0.9055 - val_vowel_accuracy: 0.9675 - val_consonant_accuracy: 0.9694\n",
      "Epoch 65/100\n",
      "180756/180756 [==============================] - 90s 498us/sample - loss: 0.4315 - root_loss: 0.2469 - vowel_loss: 0.0928 - consonant_loss: 0.0918 - root_accuracy: 0.9229 - vowel_accuracy: 0.9718 - consonant_accuracy: 0.9707 - val_loss: 0.6196 - val_root_loss: 0.3835 - val_vowel_loss: 0.1117 - val_consonant_loss: 0.1242 - val_root_accuracy: 0.9049 - val_vowel_accuracy: 0.9698 - val_consonant_accuracy: 0.9644\n",
      "Epoch 66/100\n",
      "180756/180756 [==============================] - 90s 496us/sample - loss: 0.4313 - root_loss: 0.2476 - vowel_loss: 0.0921 - consonant_loss: 0.0916 - root_accuracy: 0.9224 - vowel_accuracy: 0.9724 - consonant_accuracy: 0.9709 - val_loss: 0.6149 - val_root_loss: 0.3950 - val_vowel_loss: 0.1094 - val_consonant_loss: 0.1103 - val_root_accuracy: 0.9050 - val_vowel_accuracy: 0.9717 - val_consonant_accuracy: 0.9686\n",
      "Epoch 67/100\n",
      "180756/180756 [==============================] - 89s 491us/sample - loss: 0.4282 - root_loss: 0.2465 - vowel_loss: 0.0914 - consonant_loss: 0.0903 - root_accuracy: 0.9233 - vowel_accuracy: 0.9724 - consonant_accuracy: 0.9713 - val_loss: 0.6173 - val_root_loss: 0.3893 - val_vowel_loss: 0.1107 - val_consonant_loss: 0.1169 - val_root_accuracy: 0.9071 - val_vowel_accuracy: 0.9711 - val_consonant_accuracy: 0.9686\n",
      "Epoch 68/100\n",
      "180756/180756 [==============================] - 88s 487us/sample - loss: 0.4279 - root_loss: 0.2464 - vowel_loss: 0.0908 - consonant_loss: 0.0908 - root_accuracy: 0.9229 - vowel_accuracy: 0.9724 - consonant_accuracy: 0.9713 - val_loss: 0.6386 - val_root_loss: 0.4067 - val_vowel_loss: 0.1200 - val_consonant_loss: 0.1115 - val_root_accuracy: 0.9052 - val_vowel_accuracy: 0.9697 - val_consonant_accuracy: 0.9692\n",
      "Epoch 69/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4281 - root_loss: 0.2460 - vowel_loss: 0.0911 - consonant_loss: 0.0911 - root_accuracy: 0.9236 - vowel_accuracy: 0.9723 - consonant_accuracy: 0.9712 - val_loss: 0.6426 - val_root_loss: 0.4098 - val_vowel_loss: 0.1167 - val_consonant_loss: 0.1158 - val_root_accuracy: 0.9073 - val_vowel_accuracy: 0.9703 - val_consonant_accuracy: 0.9678\n",
      "Epoch 70/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4259 - root_loss: 0.2441 - vowel_loss: 0.0917 - consonant_loss: 0.0902 - root_accuracy: 0.9234 - vowel_accuracy: 0.9723 - consonant_accuracy: 0.9715 - val_loss: 0.6388 - val_root_loss: 0.4047 - val_vowel_loss: 0.1196 - val_consonant_loss: 0.1142 - val_root_accuracy: 0.9062 - val_vowel_accuracy: 0.9694 - val_consonant_accuracy: 0.9688\n",
      "Epoch 71/100\n",
      "180756/180756 [==============================] - 88s 486us/sample - loss: 0.4247 - root_loss: 0.2449 - vowel_loss: 0.0911 - consonant_loss: 0.0887 - root_accuracy: 0.9235 - vowel_accuracy: 0.9723 - consonant_accuracy: 0.9719 - val_loss: 0.6092 - val_root_loss: 0.3909 - val_vowel_loss: 0.1107 - val_consonant_loss: 0.1074 - val_root_accuracy: 0.9100 - val_vowel_accuracy: 0.9718 - val_consonant_accuracy: 0.9713\n",
      "Epoch 72/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4229 - root_loss: 0.2425 - vowel_loss: 0.0905 - consonant_loss: 0.0899 - root_accuracy: 0.9243 - vowel_accuracy: 0.9733 - consonant_accuracy: 0.9712 - val_loss: 0.6251 - val_root_loss: 0.3907 - val_vowel_loss: 0.1143 - val_consonant_loss: 0.1199 - val_root_accuracy: 0.9065 - val_vowel_accuracy: 0.9712 - val_consonant_accuracy: 0.9675\n",
      "Epoch 73/100\n",
      "180756/180756 [==============================] - 88s 488us/sample - loss: 0.4220 - root_loss: 0.2427 - vowel_loss: 0.0905 - consonant_loss: 0.0887 - root_accuracy: 0.9242 - vowel_accuracy: 0.9727 - consonant_accuracy: 0.9722 - val_loss: 0.6359 - val_root_loss: 0.3999 - val_vowel_loss: 0.1198 - val_consonant_loss: 0.1158 - val_root_accuracy: 0.9098 - val_vowel_accuracy: 0.9692 - val_consonant_accuracy: 0.9691\n",
      "Epoch 74/100\n",
      "180756/180756 [==============================] - 88s 486us/sample - loss: 0.4211 - root_loss: 0.2417 - vowel_loss: 0.0904 - consonant_loss: 0.0890 - root_accuracy: 0.9252 - vowel_accuracy: 0.9729 - consonant_accuracy: 0.9721 - val_loss: 0.6393 - val_root_loss: 0.4108 - val_vowel_loss: 0.1144 - val_consonant_loss: 0.1139 - val_root_accuracy: 0.9027 - val_vowel_accuracy: 0.9699 - val_consonant_accuracy: 0.9692\n",
      "Epoch 75/100\n",
      "180756/180756 [==============================] - 88s 485us/sample - loss: 0.4211 - root_loss: 0.2428 - vowel_loss: 0.0903 - consonant_loss: 0.0879 - root_accuracy: 0.9241 - vowel_accuracy: 0.9729 - consonant_accuracy: 0.9721 - val_loss: 0.6080 - val_root_loss: 0.3828 - val_vowel_loss: 0.1088 - val_consonant_loss: 0.1161 - val_root_accuracy: 0.9142 - val_vowel_accuracy: 0.9706 - val_consonant_accuracy: 0.9684\n",
      "Epoch 76/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4192 - root_loss: 0.2402 - vowel_loss: 0.0885 - consonant_loss: 0.0905 - root_accuracy: 0.9246 - vowel_accuracy: 0.9730 - consonant_accuracy: 0.9713 - val_loss: 0.6024 - val_root_loss: 0.3728 - val_vowel_loss: 0.1126 - val_consonant_loss: 0.1167 - val_root_accuracy: 0.9094 - val_vowel_accuracy: 0.9698 - val_consonant_accuracy: 0.9676\n",
      "Epoch 77/100\n",
      "180756/180756 [==============================] - 88s 489us/sample - loss: 0.4218 - root_loss: 0.2410 - vowel_loss: 0.0912 - consonant_loss: 0.0896 - root_accuracy: 0.9246 - vowel_accuracy: 0.9720 - consonant_accuracy: 0.9717 - val_loss: 0.6107 - val_root_loss: 0.3860 - val_vowel_loss: 0.1127 - val_consonant_loss: 0.1117 - val_root_accuracy: 0.9077 - val_vowel_accuracy: 0.9718 - val_consonant_accuracy: 0.9694\n",
      "Epoch 78/100\n",
      "180756/180756 [==============================] - 88s 484us/sample - loss: 0.4169 - root_loss: 0.2399 - vowel_loss: 0.0884 - consonant_loss: 0.0887 - root_accuracy: 0.9244 - vowel_accuracy: 0.9729 - consonant_accuracy: 0.9718 - val_loss: 0.6316 - val_root_loss: 0.4095 - val_vowel_loss: 0.1123 - val_consonant_loss: 0.1095 - val_root_accuracy: 0.9045 - val_vowel_accuracy: 0.9715 - val_consonant_accuracy: 0.9698\n",
      "Epoch 79/100\n",
      "180756/180756 [==============================] - 87s 479us/sample - loss: 0.4136 - root_loss: 0.2386 - vowel_loss: 0.0868 - consonant_loss: 0.0882 - root_accuracy: 0.9252 - vowel_accuracy: 0.9739 - consonant_accuracy: 0.9718 - val_loss: 0.6585 - val_root_loss: 0.4121 - val_vowel_loss: 0.1289 - val_consonant_loss: 0.1171 - val_root_accuracy: 0.9051 - val_vowel_accuracy: 0.9666 - val_consonant_accuracy: 0.9688\n",
      "Epoch 80/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.4160 - root_loss: 0.2385 - vowel_loss: 0.0888 - consonant_loss: 0.0887 - root_accuracy: 0.9251 - vowel_accuracy: 0.9734 - consonant_accuracy: 0.9718 - val_loss: 0.6308 - val_root_loss: 0.3904 - val_vowel_loss: 0.1241 - val_consonant_loss: 0.1160 - val_root_accuracy: 0.9048 - val_vowel_accuracy: 0.9679 - val_consonant_accuracy: 0.9678\n",
      "Epoch 81/100\n",
      "180756/180756 [==============================] - 87s 480us/sample - loss: 0.3456 - root_loss: 0.1924 - vowel_loss: 0.0759 - consonant_loss: 0.0773 - root_accuracy: 0.9388 - vowel_accuracy: 0.9774 - consonant_accuracy: 0.9755 - val_loss: 0.5857 - val_root_loss: 0.3739 - val_vowel_loss: 0.1044 - val_consonant_loss: 0.1072 - val_root_accuracy: 0.9143 - val_vowel_accuracy: 0.9736 - val_consonant_accuracy: 0.9718\n",
      "Epoch 82/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.3305 - root_loss: 0.1831 - vowel_loss: 0.0735 - consonant_loss: 0.0738 - root_accuracy: 0.9424 - vowel_accuracy: 0.9778 - consonant_accuracy: 0.9765 - val_loss: 0.6109 - val_root_loss: 0.3919 - val_vowel_loss: 0.1073 - val_consonant_loss: 0.1114 - val_root_accuracy: 0.9111 - val_vowel_accuracy: 0.9731 - val_consonant_accuracy: 0.9710\n",
      "Epoch 83/100\n",
      "180756/180756 [==============================] - 87s 481us/sample - loss: 0.3284 - root_loss: 0.1819 - vowel_loss: 0.0724 - consonant_loss: 0.0741 - root_accuracy: 0.9420 - vowel_accuracy: 0.9785 - consonant_accuracy: 0.9769 - val_loss: 0.5924 - val_root_loss: 0.3798 - val_vowel_loss: 0.1054 - val_consonant_loss: 0.1069 - val_root_accuracy: 0.9153 - val_vowel_accuracy: 0.9727 - val_consonant_accuracy: 0.9721\n",
      "Epoch 84/100\n",
      "180756/180756 [==============================] - 87s 480us/sample - loss: 0.3257 - root_loss: 0.1796 - vowel_loss: 0.0726 - consonant_loss: 0.0735 - root_accuracy: 0.9430 - vowel_accuracy: 0.9782 - consonant_accuracy: 0.9770 - val_loss: 0.5890 - val_root_loss: 0.3782 - val_vowel_loss: 0.1044 - val_consonant_loss: 0.1061 - val_root_accuracy: 0.9146 - val_vowel_accuracy: 0.9741 - val_consonant_accuracy: 0.9719\n",
      "Epoch 85/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.3256 - root_loss: 0.1808 - vowel_loss: 0.0716 - consonant_loss: 0.0732 - root_accuracy: 0.9426 - vowel_accuracy: 0.9786 - consonant_accuracy: 0.9772 - val_loss: 0.5902 - val_root_loss: 0.3798 - val_vowel_loss: 0.1054 - val_consonant_loss: 0.1047 - val_root_accuracy: 0.9138 - val_vowel_accuracy: 0.9733 - val_consonant_accuracy: 0.9724\n",
      "Epoch 86/100\n",
      "180756/180756 [==============================] - 86s 474us/sample - loss: 0.3234 - root_loss: 0.1779 - vowel_loss: 0.0721 - consonant_loss: 0.0734 - root_accuracy: 0.9433 - vowel_accuracy: 0.9781 - consonant_accuracy: 0.9769 - val_loss: 0.5976 - val_root_loss: 0.3844 - val_vowel_loss: 0.1084 - val_consonant_loss: 0.1045 - val_root_accuracy: 0.9136 - val_vowel_accuracy: 0.9726 - val_consonant_accuracy: 0.9730\n",
      "Epoch 87/100\n",
      "180756/180756 [==============================] - 86s 474us/sample - loss: 0.3206 - root_loss: 0.1769 - vowel_loss: 0.0704 - consonant_loss: 0.0733 - root_accuracy: 0.9442 - vowel_accuracy: 0.9787 - consonant_accuracy: 0.9770 - val_loss: 0.5874 - val_root_loss: 0.3788 - val_vowel_loss: 0.1041 - val_consonant_loss: 0.1041 - val_root_accuracy: 0.9150 - val_vowel_accuracy: 0.9737 - val_consonant_accuracy: 0.9725\n",
      "Epoch 88/100\n",
      "180756/180756 [==============================] - 87s 480us/sample - loss: 0.3193 - root_loss: 0.1763 - vowel_loss: 0.0709 - consonant_loss: 0.0720 - root_accuracy: 0.9441 - vowel_accuracy: 0.9787 - consonant_accuracy: 0.9769 - val_loss: 0.5736 - val_root_loss: 0.3665 - val_vowel_loss: 0.1036 - val_consonant_loss: 0.1031 - val_root_accuracy: 0.9171 - val_vowel_accuracy: 0.9749 - val_consonant_accuracy: 0.9730\n",
      "Epoch 89/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.3193 - root_loss: 0.1762 - vowel_loss: 0.0708 - consonant_loss: 0.0723 - root_accuracy: 0.9439 - vowel_accuracy: 0.9787 - consonant_accuracy: 0.9771 - val_loss: 0.5854 - val_root_loss: 0.3769 - val_vowel_loss: 0.1038 - val_consonant_loss: 0.1044 - val_root_accuracy: 0.9167 - val_vowel_accuracy: 0.9736 - val_consonant_accuracy: 0.9725\n",
      "Epoch 90/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.3173 - root_loss: 0.1753 - vowel_loss: 0.0700 - consonant_loss: 0.0720 - root_accuracy: 0.9438 - vowel_accuracy: 0.9794 - consonant_accuracy: 0.9771 - val_loss: 0.5873 - val_root_loss: 0.3780 - val_vowel_loss: 0.1025 - val_consonant_loss: 0.1066 - val_root_accuracy: 0.9156 - val_vowel_accuracy: 0.9746 - val_consonant_accuracy: 0.9714\n",
      "Epoch 91/100\n",
      "180756/180756 [==============================] - 86s 477us/sample - loss: 0.3178 - root_loss: 0.1755 - vowel_loss: 0.0710 - consonant_loss: 0.0713 - root_accuracy: 0.9443 - vowel_accuracy: 0.9787 - consonant_accuracy: 0.9772 - val_loss: 0.5851 - val_root_loss: 0.3744 - val_vowel_loss: 0.1041 - val_consonant_loss: 0.1064 - val_root_accuracy: 0.9142 - val_vowel_accuracy: 0.9741 - val_consonant_accuracy: 0.9725\n",
      "Epoch 92/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.3169 - root_loss: 0.1750 - vowel_loss: 0.0704 - consonant_loss: 0.0715 - root_accuracy: 0.9439 - vowel_accuracy: 0.9783 - consonant_accuracy: 0.9774 - val_loss: 0.5822 - val_root_loss: 0.3714 - val_vowel_loss: 0.1063 - val_consonant_loss: 0.1042 - val_root_accuracy: 0.9147 - val_vowel_accuracy: 0.9739 - val_consonant_accuracy: 0.9727\n",
      "Epoch 93/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.3172 - root_loss: 0.1745 - vowel_loss: 0.0705 - consonant_loss: 0.0722 - root_accuracy: 0.9441 - vowel_accuracy: 0.9791 - consonant_accuracy: 0.9769 - val_loss: 0.5884 - val_root_loss: 0.3770 - val_vowel_loss: 0.1030 - val_consonant_loss: 0.1081 - val_root_accuracy: 0.9155 - val_vowel_accuracy: 0.9743 - val_consonant_accuracy: 0.9715\n",
      "Epoch 94/100\n",
      "180756/180756 [==============================] - 86s 477us/sample - loss: 0.3117 - root_loss: 0.1717 - vowel_loss: 0.0692 - consonant_loss: 0.0708 - root_accuracy: 0.9450 - vowel_accuracy: 0.9792 - consonant_accuracy: 0.9773 - val_loss: 0.5952 - val_root_loss: 0.3843 - val_vowel_loss: 0.1053 - val_consonant_loss: 0.1052 - val_root_accuracy: 0.9137 - val_vowel_accuracy: 0.9735 - val_consonant_accuracy: 0.9730\n",
      "Epoch 95/100\n",
      "180756/180756 [==============================] - 86s 478us/sample - loss: 0.3149 - root_loss: 0.1735 - vowel_loss: 0.0696 - consonant_loss: 0.0717 - root_accuracy: 0.9444 - vowel_accuracy: 0.9792 - consonant_accuracy: 0.9771 - val_loss: 0.6073 - val_root_loss: 0.3929 - val_vowel_loss: 0.1079 - val_consonant_loss: 0.1061 - val_root_accuracy: 0.9118 - val_vowel_accuracy: 0.9735 - val_consonant_accuracy: 0.9720\n",
      "Epoch 96/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.3130 - root_loss: 0.1717 - vowel_loss: 0.0702 - consonant_loss: 0.0711 - root_accuracy: 0.9452 - vowel_accuracy: 0.9788 - consonant_accuracy: 0.9778 - val_loss: 0.6028 - val_root_loss: 0.3902 - val_vowel_loss: 0.1045 - val_consonant_loss: 0.1077 - val_root_accuracy: 0.9139 - val_vowel_accuracy: 0.9741 - val_consonant_accuracy: 0.9718\n",
      "Epoch 97/100\n",
      "180756/180756 [==============================] - 86s 475us/sample - loss: 0.3117 - root_loss: 0.1709 - vowel_loss: 0.0698 - consonant_loss: 0.0710 - root_accuracy: 0.9455 - vowel_accuracy: 0.9792 - consonant_accuracy: 0.9774 - val_loss: 0.5933 - val_root_loss: 0.3813 - val_vowel_loss: 0.1056 - val_consonant_loss: 0.1062 - val_root_accuracy: 0.9150 - val_vowel_accuracy: 0.9735 - val_consonant_accuracy: 0.9720\n",
      "Epoch 98/100\n",
      "180756/180756 [==============================] - 86s 473us/sample - loss: 0.3128 - root_loss: 0.1718 - vowel_loss: 0.0696 - consonant_loss: 0.0714 - root_accuracy: 0.9447 - vowel_accuracy: 0.9791 - consonant_accuracy: 0.9776 - val_loss: 0.5998 - val_root_loss: 0.3884 - val_vowel_loss: 0.1071 - val_consonant_loss: 0.1041 - val_root_accuracy: 0.9146 - val_vowel_accuracy: 0.9739 - val_consonant_accuracy: 0.9728\n",
      "Epoch 99/100\n",
      "180756/180756 [==============================] - 86s 476us/sample - loss: 0.3132 - root_loss: 0.1739 - vowel_loss: 0.0698 - consonant_loss: 0.0695 - root_accuracy: 0.9444 - vowel_accuracy: 0.9790 - consonant_accuracy: 0.9775 - val_loss: 0.6023 - val_root_loss: 0.3874 - val_vowel_loss: 0.1069 - val_consonant_loss: 0.1077 - val_root_accuracy: 0.9124 - val_vowel_accuracy: 0.9735 - val_consonant_accuracy: 0.9719\n",
      "Epoch 100/100\n",
      "180756/180756 [==============================] - 85s 472us/sample - loss: 0.2985 - root_loss: 0.1635 - vowel_loss: 0.0667 - consonant_loss: 0.0683 - root_accuracy: 0.9477 - vowel_accuracy: 0.9801 - consonant_accuracy: 0.9787 - val_loss: 0.5867 - val_root_loss: 0.3764 - val_vowel_loss: 0.1044 - val_consonant_loss: 0.1056 - val_root_accuracy: 0.9163 - val_vowel_accuracy: 0.9741 - val_consonant_accuracy: 0.9723\n"
     ]
    }
   ],
   "source": [
    "history_sayannet_v4 = model_sayannet_v4.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=100, callbacks=get_callbacks('sayannet_v4_noAug.h5','sayannet_v4_noAug.h5',32,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l94lT4gRGMZ1"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v4.save('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v4_noAug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeIRs3W6G5Dh"
   },
   "source": [
    "### SayanNet v5 - Fractal Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ix_A-2RTH6mJ"
   },
   "outputs": [],
   "source": [
    "def model_sayannet_v5():\n",
    "  \n",
    "  deepest=False\n",
    "  dropout = [0., 0.1, 0.2, 0.3, 0.4]\n",
    "  conv = [(64, 3, 3), (128, 3, 3), (256, 3, 3), (512, 3, 3), (512, 2, 2)]\n",
    "\n",
    "  input_= Input(shape=(64, 64,1))\n",
    "  #augmentation = Lambda(apply_augmentation, output_shape=(64,64,1))(input_)\n",
    "  output = fractal_net(c=3, b=5, conv=conv,drop_path=0.15, \n",
    "                       dropout=dropout,deepest=deepest)(input_)\n",
    "  \n",
    "  flat = Flatten()(output)\n",
    "\n",
    "  output_root = Dense(168,activation='softmax',name='root')(flat)\n",
    "\n",
    "  output_vowel = Dense(11,activation='softmax',name='vowel')(flat)\n",
    "  \n",
    "  output_consonant = Dense(7,activation='softmax',name='consonant')(flat)\n",
    "\n",
    "  model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AKJro3QhIHn"
   },
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "model_sayannet_v5 = model_sayannet_v5()\n",
    "keras.utils.plot_model(model_sayannet_v5, '/content/drive/My Drive/Bengali Grapheme/results/sayannet_v5.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOikmydAi9e7"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v5.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = 'Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1729,
     "status": "error",
     "timestamp": 1583211769538,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "XXAdO1OmjHW9",
    "outputId": "ef6833aa-851f-4361-daec-5fc66bd8a04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180756 samples, validate on 20084 samples\n",
      "Epoch 1/30\n",
      "    32/180756 [..............................] - ETA: 8:23WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `root_loss` which is not available. Available metrics are: lr\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `vowel_loss` which is not available. Available metrics are: lr\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `consonant_loss` which is not available. Available metrics are: lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `root_loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-284d7a792bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                     \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                     batch_size=32, epochs=30, callbacks=get_callbacks('sayannet_v5.h5','sayannet_v5.h5',32,30))\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_feed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 85\u001b[0;31m         per_replica_function, args=args)\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    762\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2164\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    254\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d59f242c9ba1>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_train_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d59f242c9ba1>\u001b[0m in \u001b[0;36m_drop_path\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_global\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_global_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_local_drops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mave\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdrops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpy_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m     \u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m    815\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(next_creator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2078\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m       \u001b[0m_require_strategy_scope_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2080\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_creator_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(next_creator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2078\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m       \u001b[0m_require_strategy_scope_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2080\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_creator_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(next_creator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2078\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m       \u001b[0m_require_strategy_scope_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2080\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_creator_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36minvalid_creator_scope\u001b[0;34m(*unused_args, **unused_kwds)\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;34m\"\"\"Disables variable creation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m       raise ValueError(\n\u001b[0;32m--> 502\u001b[0;31m           \u001b[0;34m\"tf.function-decorated function tried to create \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m           \"variables on non-first call.\")\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.function-decorated function tried to create variables on non-first call."
     ]
    }
   ],
   "source": [
    "history_sayannet_v5 = model_sayannet_v5.fit(X_train.reshape(-1,64,64,1),\n",
    "                                    (y_train[:,0], y_train[:,1],y_train[:,2]), \n",
    "                                    validation_data = (X_valid.reshape(-1,64,64,1), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "                                    batch_size=32, epochs=30, callbacks=get_callbacks('sayannet_v5.h5','sayannet_v5.h5',32,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Brab1MaJwBLD"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIQ4dxBUwE52"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v3 = tf.keras.models.load_model('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79753,
     "status": "ok",
     "timestamp": 1583171886573,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "E7WWvwQxYrP1",
    "outputId": "fda83aaa-4f3d-4756-c003-371abbfbfb9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66278/66278 [==============================] - 80s 1ms/sample - loss: 1.3984 - root_loss: 0.9235 - vowel_loss: 0.2498 - consonant_loss: 0.2248 - root_accuracy: 0.7737 - vowel_accuracy: 0.9274 - consonant_accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3984383012731951,\n",
       " 0.92349756,\n",
       " 0.2497758,\n",
       " 0.22484691,\n",
       " 0.77368057,\n",
       " 0.9274118,\n",
       " 0.93325084]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sayannet_v3.evaluate(X_valid.reshape(-1,64,64,1),  (y_valid[:,0], y_valid[:,1],y_valid[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lwzwn-gTY31W"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v1 = tf.keras.models.load_model('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28353,
     "status": "ok",
     "timestamp": 1583172004211,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "YysSK-9xZjMl",
    "outputId": "fcf4f8ea-7f27-4b71-9cb8-61fd492d1b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66278/66278 [==============================] - 28s 423us/sample - loss: 3.1799 - root_loss: 1.7218 - vowel_loss: 0.5916 - consonant_loss: 0.8665 - root_accuracy: 0.5346 - vowel_accuracy: 0.7960 - consonant_accuracy: 0.6696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1799310623165953,\n",
       " 1.7218243,\n",
       " 0.5915747,\n",
       " 0.8664525,\n",
       " 0.5346269,\n",
       " 0.7959806,\n",
       " 0.6696189]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sayannet_v1.evaluate(X_valid.reshape(-1,64,64,1),  (y_valid[:,0], y_valid[:,1],y_valid[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hQcJcBTZznR"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v2 = tf.keras.models.load_model('/content/drive/My Drive/Bengali Grapheme/models/sayannet_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53837,
     "status": "ok",
     "timestamp": 1583182384112,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "8pLjxWOMBRHo",
    "outputId": "826e2846-aadf-4c5c-a410-899f965a27f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66278/66278 [==============================] - 53s 806us/sample - loss: 2.4278 - root_loss: 1.4809 - vowel_loss: 0.5181 - consonant_loss: 0.4289 - root_accuracy: 0.6048 - vowel_accuracy: 0.8217 - consonant_accuracy: 0.8531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.427800789434522,\n",
       " 1.480861,\n",
       " 0.5180764,\n",
       " 0.42888618,\n",
       " 0.604801,\n",
       " 0.82173574,\n",
       " 0.8531338]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sayannet_v2.evaluate(X_valid.reshape(-1,64,64,1),  (y_valid[:,0], y_valid[:,1],y_valid[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMc_BiJlBThx"
   },
   "outputs": [],
   "source": [
    "model_sayannet_v4 = tf.keras.models.load_model('../models/sayannet_v4_noAug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17461,
     "status": "ok",
     "timestamp": 1583193113017,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "GMk135c2ppqR",
    "outputId": "0d7a7689-ebfc-4355-96e7-183b9699114f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66278/66278 [==============================] - 17s 255us/sample - loss: 0.7083 - root_loss: 0.4101 - vowel_loss: 0.1452 - consonant_loss: 0.1527 - root_accuracy: 0.8861 - vowel_accuracy: 0.9594 - consonant_accuracy: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7082605381701996,\n",
       " 0.41010314,\n",
       " 0.14515221,\n",
       " 0.15274046,\n",
       " 0.8860708,\n",
       " 0.95939827,\n",
       " 0.95589787]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sayannet_v4.evaluate(X_valid.reshape(-1,64,64,1),  (y_valid[:,0], y_valid[:,1],y_valid[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JH_FVODuqWXO"
   },
   "outputs": [],
   "source": [
    "preds = model_sayannet_v4.predict(X_valid.reshape(-1,64,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKPOqZBxq5r9"
   },
   "outputs": [],
   "source": [
    "result_root = np.zeros(66278,dtype=np.uint32)\n",
    "for i in range(preds[0].shape[0]):\n",
    "  result_root[i] = np.argmax(preds[0][i])\n",
    "y_true = y_valid[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1583194442022,
     "user": {
      "displayName": "Sayan Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64",
      "userId": "03850430243742702686"
     },
     "user_tz": 300
    },
    "id": "g3hB6l5lrgVp",
    "outputId": "f7c870f9-b7c4-4ea6-daef-1c3e2e41d384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97619048 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.09433962 0.86792453 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.84821429 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.91292135 0.00280899 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.93814433 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.91825095]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_true, result_root,normalize='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdZZucdWsj9W"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = np.array(classes)\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(30, 30)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ar6XtRf4twih"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true,result_root,classes=np.array(range(168),dtype=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LynQVH2t3Pv",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ann_visualizer.visualize import ann_viz\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def myprint(s):\n",
    "    with open('modelsummary.txt','w+') as f:\n",
    "        print(s, file=f)\n",
    "\n",
    "model_sayannet_v4.summary(print_fn=myprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 24)   216         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 24)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 24)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 24)   0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 24)   96          max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 24)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 12)   2592        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 36)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 36)   144         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 36)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 12)   3888        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 48)   0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 48)   192         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 48)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 12)   5184        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 60)   0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 60)   240         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 60)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 12)   6480        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 72)   0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 72)   288         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 72)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 72)   5184        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 72)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 72)   288         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 72)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 12)   7776        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 84)   0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 84)   336         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 84)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 12)   9072        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 12)   10368       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 108)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 108)  432         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 108)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 12)   11664       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 120)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 120)  480         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 120)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 120)  14400       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 8, 8, 120)    0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 120)    480         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 120)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 12)     12960       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 8, 8, 12)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 132)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 132)    528         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 132)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 12)     14256       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 8, 8, 12)     0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 144)    0           concatenate_20[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 144)    576         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 144)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 12)     15552       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 8, 12)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 156)    0           concatenate_21[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 156)    624         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 156)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 12)     16848       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 8, 8, 12)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 168)    0           concatenate_22[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 168)    672         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 168)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 168)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "root (Dense)                    (None, 168)          28392       global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           1859        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            1183        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 173,730\n",
      "Trainable params: 170,802\n",
      "Non-trainable params: 2,928\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_sayannet_v4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 24)   216         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 24)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 24)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 24)   0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 24)   96          max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 24)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 12)   2592        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 36)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 36)   144         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 36)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 12)   3888        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 48)   0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 48)   192         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 48)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 12)   5184        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 60)   0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 60)   240         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 60)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 12)   6480        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 72)   0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 72)   288         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 72)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 72)   5184        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 72)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 72)   288         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 72)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 12)   7776        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 84)   0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 84)   336         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 84)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 12)   9072        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 12)   10368       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 108)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 108)  432         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 108)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 12)   11664       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 120)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 120)  480         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 120)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 120)  14400       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 8, 8, 120)    0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 120)    480         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 120)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 12)     12960       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 8, 8, 12)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 132)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 132)    528         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 132)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 12)     14256       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 8, 8, 12)     0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 144)    0           concatenate_20[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 144)    576         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 144)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 12)     15552       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 8, 12)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 156)    0           concatenate_21[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 156)    624         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 156)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 12)     16848       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 8, 8, 12)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 168)    0           concatenate_22[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 168)    672         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 168)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 168)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "root (Dense)                    (None, 168)          28392       global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           1859        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            1183        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 173,730\n",
      "Trainable params: 170,802\n",
      "Non-trainable params: 2,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample = open('samplefile.txt', 'w')\n",
    "print(model_sayannet_v4.summary(), file = sample) \n",
    "sample.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cPuNwk2VcFN3",
    "sAw_dPwHcJ-S",
    "EtCVWcbVo7_M",
    "2yBeflKG6dIH",
    "yeIRs3W6G5Dh"
   ],
   "machine_shape": "hm",
   "name": "Augmentation,Generators,Architectures_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
