{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib.font_manager import FontProperties\n",
    "prop = FontProperties()\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "prop.set_file('../data/kalpurush.ttf')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras import backend as K\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/sayan/Documents/Bengali_Grapheme/data/train.csv')\n",
    "strs = train_df['image_id'].values\n",
    "strs = [string+'_f.png' for string in strs]\n",
    "train_df.drop(['image_id'],axis=1,inplace=True)\n",
    "train_df['image_id'] = strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>Train_0_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>Train_1_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>Train_2_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>Train_3_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>Train_4_f.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grapheme_root  vowel_diacritic  consonant_diacritic grapheme       image_id\n",
       "0             15                9                    5   ক্ট্রো  Train_0_f.png\n",
       "1            159                0                    0        হ  Train_1_f.png\n",
       "2             22                3                    5     খ্রী  Train_2_f.png\n",
       "3             53                2                    2     র্টি  Train_3_f.png\n",
       "4             71                9                    5     থ্রো  Train_4_f.png"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = pd.read_csv('/home/sayan/Documents/Bengali_Grapheme/data/class_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def apply_augmentation(image):\n",
    "    augmentation_pipeline = A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% of images\n",
    "                    A.RandomContrast(), # apply random contrast\n",
    "                    A.RandomGamma(), # apply random gamma\n",
    "                    A.RandomBrightness(), # apply random brightness\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% images\n",
    "                    A.ElasticTransform(alpha = 120,sigma = 120 * 0.05,alpha_affine = 120 * 0.03),\n",
    "                    A.GridDistortion(),\n",
    "                    A.OpticalDistortion(distort_limit = 2,shift_limit = 0.5),\n",
    "                    A.ShiftScaleRotate()\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% images\n",
    "                    A.GaussianBlur(alpha = 120,sigma = 120 * 0.05,alpha_affine = 120 * 0.03),\n",
    "                    A.GridDistortion(),\n",
    "                    A.OpticalDistortion(distort_limit = 2,shift_limit = 0.5),\n",
    "                    \n",
    "                    \n",
    "                ],\n",
    "                p = 0.5\n",
    "            )\n",
    "        ],\n",
    "        p = 0.5\n",
    "    )\n",
    "    images_aug = augmentation_pipeline(image = image)['image']\n",
    "    return images_aug\n",
    "\n",
    "def image_scale(image):\n",
    "    augmentation_pipeline = A.Resize(224,224,always_apply=True)\n",
    "    images_aug = augmentation_pipeline(image = image)['image']\n",
    "    \n",
    "    return images_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modified_recall(y_true, y_pred):\n",
    "    \n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[solution[component] == component]['target'].values\n",
    "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    \n",
    "    final_score = np.average(scores, weights=[2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_toy_processed_images = '/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images'\n",
    "train_df_sample = pd.DataFrame()\n",
    "for (directory, _ , image_names) in os.walk(datadir_toy_processed_images):\n",
    "        train_df_sample = train_df[train_df['image_id'].isin(image_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20084, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_1, valid_df_1 = train_test_split(train_df_sample, random_state=42,\n",
    "                                          stratify=train_df_sample[['grapheme_root',\n",
    "                                                                    'vowel_diacritic',\n",
    "                                                                    'consonant_diacritic']])\n",
    "\n",
    "#train_df_1, valid_df_1 = train_test_split(train_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105777</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>প্রি</td>\n",
       "      <td>Train_105777_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106468</th>\n",
       "      <td>123</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>রে</td>\n",
       "      <td>Train_106468_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>ঠ্যা</td>\n",
       "      <td>Train_10501_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22628</th>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ফে</td>\n",
       "      <td>Train_22628_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128257</th>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>স্পী</td>\n",
       "      <td>Train_128257_f.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        grapheme_root  vowel_diacritic  consonant_diacritic grapheme  \\\n",
       "105777             96                2                    5     প্রি   \n",
       "106468            123                7                    0       রে   \n",
       "10501              55                1                    4     ঠ্যা   \n",
       "22628             103                7                    0       ফে   \n",
       "128257            153                3                    0     স্পী   \n",
       "\n",
       "                  image_id  \n",
       "105777  Train_105777_f.png  \n",
       "106468  Train_106468_f.png  \n",
       "10501    Train_10501_f.png  \n",
       "22628    Train_22628_f.png  \n",
       "128257  Train_128257_f.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def oneHotEncode_outputs(gen_image):\n",
    "    while True:\n",
    "        output = next(gen_image)\n",
    "        img = output[0]\n",
    "        labels = output[1]\n",
    "        #grapheme_label = np.zeros(168)\n",
    "        #vowel_label = np.zeros(11)\n",
    "        #consonant_label = np.zeros(7)\n",
    "        #for i in range(3):\n",
    "        #    if i == 0:\n",
    "        #        a = labels[i]\n",
    "        #        b = np.zeros((a.size, 168))\n",
    "        #        b[np.arange(a.size),a] = 1\n",
    "        #        labels[i] = a\n",
    "        #    elif i == 1:\n",
    "        #        a = labels[i]\n",
    "        #        b = np.zeros((a.size, 11))\n",
    "        #        b[np.arange(a.size),a] = 1\n",
    "        #        labels[i] = a\n",
    "        #    else:\n",
    "        #        a = labels[i]\n",
    "        #        b = np.zeros((a.size, 7))\n",
    "        #        b[np.arange(a.size),a] = 1\n",
    "        #        labels[i] = a\n",
    "        yield (img, [np.array(labels[:,0],dtype=np.uint8), np.array(labels[:,1],dtype=np.uint8), np.array(labels[:,2],dtype=np.uint8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_image_generator():\n",
    "    \n",
    "    train_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    valid_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    train_gen_image = train_im_generator.flow_from_dataframe(train_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   class_mode = 'raw',\n",
    "                                                   target_size = (64,64),\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 1,\n",
    "                                                   seed=42)\n",
    "    valid_gen_image = valid_im_generator.flow_from_dataframe(valid_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                                 target_size = (64,64),\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   class_mode = 'raw',\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 1,\n",
    "                                                   seed=42)\n",
    "        \n",
    "    #return (train_gen_image, valid_gen_image)\n",
    "    return (oneHotEncode_outputs(train_gen_image),oneHotEncode_outputs(valid_gen_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if generator labels are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def image_from_char(char):\n",
    "    HEIGHT = 236\n",
    "    WIDTH = 236\n",
    "    image = Image.new('RGB', (WIDTH, HEIGHT))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    myfont = ImageFont.truetype('../data/kalpurush.ttf', 120)\n",
    "    w, h = draw.textsize(char, font=myfont)\n",
    "    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_generators(gen_image, train_df,fname,class_map=class_map):\n",
    "    \n",
    "    output = next(gen_image)\n",
    "    fig, axs = plt.subplots(gen_image.batch_size,4, figsize=(64, 64))\n",
    "    gs1 = gridspec.GridSpec(gen_image.batch_size, 4)\n",
    "    \n",
    "    gs1.update(wspace=0.025, hspace=0.05)\n",
    "    plt.subplots_adjust(wspace=-0.8, hspace=0.1)\n",
    "    images = output[0]\n",
    "    roots = output[1][0]\n",
    "    vowels = output[1][1]\n",
    "    consonants = output[1][2]\n",
    "    \n",
    "    for i in range(gen_image.batch_size): \n",
    "        image = images[i].reshape(64,64)\n",
    "        grapheme_root = roots[i]\n",
    "        vowel_diacritic = vowels[i]\n",
    "        consonant_diacritic = consonants[i]\n",
    "        vowel_label = class_map[(class_map['component_type'] == 'vowel_diacritic') \n",
    "                            & (class_map['label'] == vowel_diacritic)]['component'].values[0]\n",
    "        consonant_label = class_map[(class_map['component_type'] == 'consonant_diacritic') \n",
    "                            & (class_map['label'] == consonant_diacritic)]['component'].values[0]\n",
    "        root_label = class_map[(class_map['component_type'] == 'grapheme_root') \n",
    "                            & (class_map['label'] == grapheme_root)]['component'].values[0]\n",
    "        \n",
    "        axs[i,0].imshow(image,cmap='Greys')\n",
    "        axs[i,0].axis('off')\n",
    "        axs[i,0].set_aspect('equal')\n",
    "        axs[i,1].imshow(image_from_char(root_label), cmap='Greys')\n",
    "        axs[i,1].axis('off')\n",
    "        axs[i,1].set_aspect('equal')\n",
    "        axs[i,2].imshow(image_from_char(vowel_label), cmap='Greys')\n",
    "        axs[i,2].axis('off')\n",
    "        axs[i,2].set_aspect('equal')\n",
    "        axs[i,3].imshow(image_from_char(consonant_label), cmap='Greys')\n",
    "        axs[i,3].axis('off')\n",
    "        axs[i,3].set_aspect('equal')\n",
    "        \n",
    "    fig.savefig('../results/'+fname+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_generators(train_gen_image,train_df_1,'full_processed_train_images',class_map);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
    " \n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.base_m = base_m\n",
    "        self.max_m = max_m\n",
    "        self.cyclical_momentum = cyclical_momentum\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.clr_iterations = 0.\n",
    "        self.cm_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
    "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
    "    \n",
    "    def cm(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            \n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
    "            return self.max_m\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "            \n",
    "        if self.cyclical_momentum == True:\n",
    "            if self.clr_iterations == 0:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            else:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            \n",
    "            \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            K.set_value(self.model.optimizer.momentum, self.cm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_callbacks(chkpoint_name, log_name, lr_monitor):\n",
    "    \n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint('../checkpoints/'+chkpoint_name,save_best_only=True)\n",
    "    \n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "    reduceLR_cb = keras.callbacks.ReduceLROnPlateau(monitor=lr_monitor, factor=0.2,\n",
    "                                                    patience=5, min_lr=0.001)\n",
    "    root_logdir = '/home/sayan/Documents/Bengali_Grapheme/logs/'\n",
    "    \n",
    "    batch_size = train_gen_image.batch_size\n",
    "    epochs = 10\n",
    "    max_lr = 0.5\n",
    "    base_lr = max_lr/10\n",
    "    max_m = 0.98\n",
    "    base_m = 0.85\n",
    "\n",
    "    cyclical_momentum = True\n",
    "    augment = True\n",
    "    cycles = 2.35\n",
    "    \n",
    "    def get_run_logdir(name):\n",
    "        import time\n",
    "        run_id = time.strftime(name)\n",
    "        return os.path.join(root_logdir,run_id)\n",
    "\n",
    "    run_logdir = get_run_logdir(log_name+'-run_%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    iterations = round(len(train_df)/batch_size*epochs)\n",
    "    iterations = list(range(0,iterations+1))\n",
    "    step_size = len(iterations)/(cycles)\n",
    "    cyclicLR_cb =  CyclicLR(base_lr=base_lr,\n",
    "                            max_lr=max_lr,\n",
    "                            step_size=step_size,\n",
    "                            max_m=max_m,\n",
    "                            base_m=base_m,\n",
    "                            cyclical_momentum=cyclical_momentum)\n",
    "    \n",
    "    return [checkpoint_cb, reduceLR_cb, tensorboard_cb, early_stopping_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model_vanilla():\n",
    "    \n",
    "    input_ = Input(shape=(64,64,1))\n",
    "    conv1 = Conv2D(filters=6, kernel_size=(3, 3), activation='relu')(input_)\n",
    "    avgpool1 = AvgPool2D()(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(avgpool1)\n",
    "    avgpool2 = AvgPool2D()(conv2)\n",
    "\n",
    "    flat = Flatten()(avgpool2)\n",
    "    \n",
    "    dense_root_1 = Dense(2000,activation='relu')(flat)\n",
    "    dense_root_2 = Dense(1500,activation='relu')(dense_root_1)\n",
    "    dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "    dense_root_4 = Dense(800,activation='relu')(dense_root_3)\n",
    "    dense_root_5 = Dense(250,activation='relu')(dense_root_4)\n",
    "    output_root = Dense(168,activation='softmax',name='root')(dense_root_4)\n",
    "    \n",
    "    dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "    dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "    output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "    dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "    dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "    output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_vanilla = model_vanilla()\n",
    "keras.utils.plot_model(model_vanilla, '../results/vanilla_model.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_vanilla.compile(loss = keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vanilla = model_vanilla.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla.save('../models/vanilla.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_alexnet():\n",
    "    \n",
    "    input_ = Input(shape=(64,64,1))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', activation='relu')(input_)\n",
    "    maxpool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv1)\n",
    "    bn1 = BatchNormalization()(maxpool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu')(bn1)\n",
    "    maxpool2 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv2)\n",
    "    bn2 = BatchNormalization()(maxpool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(bn2)\n",
    "    drop1 = Dropout(0.5)(conv3)\n",
    "    conv4 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop1)\n",
    "    drop2 = Dropout(0.5)(conv4)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop2)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense_root_1 = Dense(4096,activation='relu')(flat)\n",
    "    dense_root_2 = Dense(4096,activation='relu')(dense_root_1)\n",
    "    dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "    output_root = Dense(168,activation='softmax',name='root')(dense_root_3)\n",
    "    \n",
    "    dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "    dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "    output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "    dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "    dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "    output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_alexnet = model_alexnet()\n",
    "keras.utils.plot_model(model_alexnet, '../results/alexnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_alexnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    513/Unknown - 57s 111ms/step - loss: 384386.3462 - root_loss: 374217.6562 - vowel_loss: 5204.3413 - consonant_loss: 4961.9897 - root_accuracy: 0.0117 - vowel_accuracy: 0.1602 - consonant_accuracy: 0.6191"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-c5a3dfab1de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history_alexnet = model_alexnet.fit(train_gen_image,\n\u001b[1;32m      2\u001b[0m                                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                     epochs = 5)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_alexnet = model_alexnet.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alexnet.save('../models/alexnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def bengali_kaggle():\n",
    "    \n",
    "    input_ = Input(shape = (64, 64, 1))\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(137, 236, 1))(input_)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = BatchNormalization(momentum=0.15)(model)\n",
    "#    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "#    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(64, 64, 1))(input_)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation = \"relu\")(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    dense = Dense(512, activation = \"relu\")(model)\n",
    "    \n",
    "    head_root = Dense(168, activation = 'softmax',name='root')(dense)\n",
    "    head_vowel = Dense(11, activation = 'softmax',name='vowel')(dense)\n",
    "    head_consonant = Dense(7, activation = 'softmax',name='consonant')(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=input_, outputs=[head_root, head_vowel, head_consonant])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_kaggle = bengali_kaggle()\n",
    "keras.utils.plot_model(model_kaggle, '../results/bengali_kaggle.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kaggle.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_kaggle = model_kaggle.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5,callbacks=get_callbacks('bengali_kaggle.h5',\n",
    "                                                                       'bengali_kaggle.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kaggle.save('../models/bengali_kaggle.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_resnet = keras.applications.resnet_v2.ResNet152V2(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = keras.layers.GlobalAveragePooling2D()(base_model_resnet.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_resnet)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_resnet)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = keras.Model(inputs=base_model_resnet.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_resnet, '../results/resnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in model_resnet.layers:\n",
    "    print(str(layer)+ ' ' +str(layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_resnet = model_resnet.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('resnet50.h5','resnet50.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.save('../models/resnet50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_vggnet = keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet = keras.layers.GlobalAveragePooling2D()(base_model_vggnet.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_vggnet)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_vggnet)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_vggnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet = keras.Model(inputs=base_model_vggnet.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_vggnet, '../results/vggnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_vggnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vggnet.layers:\n",
    "    print(str(layer)+ ' ' +str(layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vggnet = model_vggnet.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('vggnet50.h5','vggnet50.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet.save('../models/vgg16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_xception = keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception = keras.layers.GlobalAveragePooling2D()(base_model_xception.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_xception)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_xception)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception = keras.Model(inputs=base_model_xception.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_xception, '../results/xception.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model_xception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_xception = model_xception.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('xception.h5','xception.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.save('../models/xception.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_inception = keras.applications.inception_v3.InceptionV3(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = keras.layers.GlobalAveragePooling2D()(base_model_inception.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_inception)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_inception)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = keras.Model(inputs=base_model_inception.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_inception, '../results/inception.png', expand_nested=False, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_inception = model_inception.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('inception.h5','inception.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception.save('../models/inception.h5')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
