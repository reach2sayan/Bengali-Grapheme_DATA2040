{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib.font_manager import FontProperties\n",
    "prop = FontProperties()\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "prop.set_file('../data/kalpurush.ttf')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras import backend as K\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/sayan/Documents/Bengali_Grapheme/data/train.csv')\n",
    "strs = train_df['image_id'].values\n",
    "strs = [string+'_f.png' for string in strs]\n",
    "train_df.drop(['image_id'],axis=1,inplace=True)\n",
    "train_df['image_id'] = strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>Train_0_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>Train_1_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>Train_2_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>Train_3_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>Train_4_f.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grapheme_root  vowel_diacritic  consonant_diacritic grapheme       image_id\n",
       "0             15                9                    5   ক্ট্রো  Train_0_f.png\n",
       "1            159                0                    0        হ  Train_1_f.png\n",
       "2             22                3                    5     খ্রী  Train_2_f.png\n",
       "3             53                2                    2     র্টি  Train_3_f.png\n",
       "4             71                9                    5     থ্রো  Train_4_f.png"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = pd.read_csv('/home/sayan/Documents/Bengali_Grapheme/data/class_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     39
    ]
   },
   "outputs": [],
   "source": [
    "def apply_augmentation(image):\n",
    "    augmentation_pipeline = A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% of images\n",
    "                    A.RandomContrast(), # apply random contrast\n",
    "                    A.RandomGamma(), # apply random gamma\n",
    "                    A.RandomBrightness(), # apply random brightness\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% images\n",
    "                    A.ElasticTransform(alpha = 120,sigma = 120 * 0.05,alpha_affine = 120 * 0.03),\n",
    "                    A.GridDistortion(),\n",
    "                    A.OpticalDistortion(distort_limit = 2,shift_limit = 0.5),\n",
    "                    A.ShiftScaleRotate()\n",
    "                ],\n",
    "                p = 0.5\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    # apply one of transforms to 50% images\n",
    "                    A.GaussianBlur(alpha = 120,sigma = 120 * 0.05,alpha_affine = 120 * 0.03),\n",
    "                    A.GridDistortion(),\n",
    "                    A.OpticalDistortion(distort_limit = 2,shift_limit = 0.5),\n",
    "                    \n",
    "                    \n",
    "                ],\n",
    "                p = 0.5\n",
    "            )\n",
    "        ],\n",
    "        p = 0.5\n",
    "    )\n",
    "    images_aug = augmentation_pipeline(image = image)['image']\n",
    "    return images_aug\n",
    "\n",
    "def image_scale(image):\n",
    "    augmentation_pipeline = A.Resize(224,224,always_apply=True)\n",
    "    images_aug = augmentation_pipeline(image = image)['image']\n",
    "    \n",
    "    return images_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modified_recall(y_true, y_pred):\n",
    "    \n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[solution[component] == component]['target'].values\n",
    "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    \n",
    "    final_score = np.average(scores, weights=[2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_toy_processed_images = '/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images'\n",
    "train_df_sample = pd.DataFrame()\n",
    "for (directory, _ , image_names) in os.walk(datadir_toy_processed_images):\n",
    "        train_df_sample = train_df[train_df['image_id'].isin(image_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20084, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_1, valid_df_1 = train_test_split(train_df_sample, random_state=42,\n",
    "                                          stratify=train_df_sample[['grapheme_root',\n",
    "                                                                    'vowel_diacritic',\n",
    "                                                                    'consonant_diacritic']])\n",
    "\n",
    "#train_df_1, valid_df_1 = train_test_split(train_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105777</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>প্রি</td>\n",
       "      <td>Train_105777_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106468</th>\n",
       "      <td>123</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>রে</td>\n",
       "      <td>Train_106468_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>ঠ্যা</td>\n",
       "      <td>Train_10501_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22628</th>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ফে</td>\n",
       "      <td>Train_22628_f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128257</th>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>স্পী</td>\n",
       "      <td>Train_128257_f.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        grapheme_root  vowel_diacritic  consonant_diacritic grapheme  \\\n",
       "105777             96                2                    5     প্রি   \n",
       "106468            123                7                    0       রে   \n",
       "10501              55                1                    4     ঠ্যা   \n",
       "22628             103                7                    0       ফে   \n",
       "128257            153                3                    0     স্পী   \n",
       "\n",
       "                  image_id  \n",
       "105777  Train_105777_f.png  \n",
       "106468  Train_106468_f.png  \n",
       "10501    Train_10501_f.png  \n",
       "22628    Train_22628_f.png  \n",
       "128257  Train_128257_f.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_image_generator(scale_flag):\n",
    "    \n",
    "    if scale_flag:\n",
    "        train_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, \n",
    "                                                                             preprocessing_function = image_scale)\n",
    "        valid_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                                             preprocessing_function = image_scale)\n",
    "        train_gen_image = train_im_generator.flow_from_dataframe(train_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   target_size = (224,224),\n",
    "                                                   class_mode = 'multi_output',\n",
    "                                                   #color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 10,\n",
    "                                                   seed=42)\n",
    "        valid_gen_image = valid_im_generator.flow_from_dataframe(valid_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   target_size = (224,224),\n",
    "                                                   class_mode = 'multi_output',\n",
    "                                                   #color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 10,\n",
    "                                                   seed=42)\n",
    "        \n",
    "    else:\n",
    "        train_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "        valid_im_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "        \n",
    "        train_gen_image = train_im_generator.flow_from_dataframe(train_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   target_size = (64,64),\n",
    "                                                   class_mode = 'multi_output',\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 10,\n",
    "                                                   seed=42)\n",
    "        valid_gen_image = valid_im_generator.flow_from_dataframe(valid_df_1,\n",
    "                                                   directory='/home/sayan/Documents/Bengali_Grapheme/images/toy_processed_images',\n",
    "                                                   x_col='image_id',\n",
    "                                                   y_col=['grapheme_root','vowel_diacritic','consonant_diacritic'],\n",
    "                                                   target_size = (64,64),\n",
    "                                                   class_mode = 'multi_output',\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size = 10,\n",
    "                                                   seed=42)\n",
    "        \n",
    "    return (train_gen_image, valid_gen_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oneHotEncode_outputs(gen_image):\n",
    "    while True:\n",
    "        output = next(gen_image)\n",
    "        img = output[0]\n",
    "        labels = output[1]\n",
    "        #grapheme_label = np.zeros(168)\n",
    "        #vowel_label = np.zeros(11)\n",
    "        #consonant_label = np.zeros(7)\n",
    "        for i in range(3):\n",
    "            if i == 0:\n",
    "                a = labels[i]\n",
    "                b = np.zeros((a.size, 168))\n",
    "                b[np.arange(a.size),a] = 1\n",
    "                labels[i] = b\n",
    "            elif i == 1:\n",
    "                a = labels[i]\n",
    "                b = np.zeros((a.size, 11))\n",
    "                b[np.arange(a.size),a] = 1\n",
    "                labels[i] = b\n",
    "            else:\n",
    "                a = labels[i]\n",
    "                b = np.zeros((a.size, 7))\n",
    "                b[np.arange(a.size),a] = 1\n",
    "                labels[i] = b\n",
    "        yield (img, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if generator labels are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def image_from_char(char):\n",
    "    HEIGHT = 236\n",
    "    WIDTH = 236\n",
    "    image = Image.new('RGB', (WIDTH, HEIGHT))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    myfont = ImageFont.truetype('../data/kalpurush.ttf', 120)\n",
    "    w, h = draw.textsize(char, font=myfont)\n",
    "    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_generators(gen_image, train_df,fname,class_map=class_map):\n",
    "    \n",
    "    output = next(gen_image)\n",
    "    fig, axs = plt.subplots(gen_image.batch_size,4, figsize=(64, 64))\n",
    "    gs1 = gridspec.GridSpec(gen_image.batch_size, 4)\n",
    "    \n",
    "    gs1.update(wspace=0.025, hspace=0.05)\n",
    "    plt.subplots_adjust(wspace=-0.8, hspace=0.1)\n",
    "    images = output[0]\n",
    "    roots = output[1][0]\n",
    "    vowels = output[1][1]\n",
    "    consonants = output[1][2]\n",
    "    \n",
    "    for i in range(gen_image.batch_size): \n",
    "        image = images[i].reshape(64,64)\n",
    "        grapheme_root = roots[i]\n",
    "        vowel_diacritic = vowels[i]\n",
    "        consonant_diacritic = consonants[i]\n",
    "        vowel_label = class_map[(class_map['component_type'] == 'vowel_diacritic') \n",
    "                            & (class_map['label'] == vowel_diacritic)]['component'].values[0]\n",
    "        consonant_label = class_map[(class_map['component_type'] == 'consonant_diacritic') \n",
    "                            & (class_map['label'] == consonant_diacritic)]['component'].values[0]\n",
    "        root_label = class_map[(class_map['component_type'] == 'grapheme_root') \n",
    "                            & (class_map['label'] == grapheme_root)]['component'].values[0]\n",
    "        \n",
    "        axs[i,0].imshow(image,cmap='Greys')\n",
    "        axs[i,0].axis('off')\n",
    "        axs[i,0].set_aspect('equal')\n",
    "        axs[i,1].imshow(image_from_char(root_label), cmap='Greys')\n",
    "        axs[i,1].axis('off')\n",
    "        axs[i,1].set_aspect('equal')\n",
    "        axs[i,2].imshow(image_from_char(vowel_label), cmap='Greys')\n",
    "        axs[i,2].axis('off')\n",
    "        axs[i,2].set_aspect('equal')\n",
    "        axs[i,3].imshow(image_from_char(consonant_label), cmap='Greys')\n",
    "        axs[i,3].axis('off')\n",
    "        axs[i,3].set_aspect('equal')\n",
    "        \n",
    "    fig.savefig('../results/'+fname+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_generators(train_gen_image,train_df_1,'full_processed_train_images',class_map);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
    " \n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.base_m = base_m\n",
    "        self.max_m = max_m\n",
    "        self.cyclical_momentum = cyclical_momentum\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.clr_iterations = 0.\n",
    "        self.cm_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
    "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
    "    \n",
    "    def cm(self):\n",
    "        \n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        \n",
    "        if cycle == 2:\n",
    "            \n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
    "            return self.max_m\n",
    "        \n",
    "        else:\n",
    "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "            \n",
    "        if self.cyclical_momentum == True:\n",
    "            if self.clr_iterations == 0:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            else:\n",
    "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
    "            \n",
    "            \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "        if self.cyclical_momentum == True:\n",
    "            K.set_value(self.model.optimizer.momentum, self.cm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_callbacks(chkpoint_name, log_name, lr_monitor):\n",
    "    \n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint('../checkpoints/'+chkpoint_name,save_best_only=True)\n",
    "    \n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "    reduceLR_cb = keras.callbacks.ReduceLROnPlateau(monitor=lr_monitor, factor=0.2,\n",
    "                                                    patience=5, min_lr=0.001)\n",
    "    root_logdir = '/home/sayan/Documents/Bengali_Grapheme/logs/'\n",
    "    \n",
    "    batch_size = train_gen_image.batch_size\n",
    "    epochs = 10\n",
    "    max_lr = 0.5\n",
    "    base_lr = max_lr/10\n",
    "    max_m = 0.98\n",
    "    base_m = 0.85\n",
    "\n",
    "    cyclical_momentum = True\n",
    "    augment = True\n",
    "    cycles = 2.35\n",
    "    \n",
    "    def get_run_logdir(name):\n",
    "        import time\n",
    "        run_id = time.strftime(name)\n",
    "        return os.path.join(root_logdir,run_id)\n",
    "\n",
    "    run_logdir = get_run_logdir(log_name+'-run_%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    iterations = round(len(train_df)/batch_size*epochs)\n",
    "    iterations = list(range(0,iterations+1))\n",
    "    step_size = len(iterations)/(cycles)\n",
    "    cyclicLR_cb =  CyclicLR(base_lr=base_lr,\n",
    "                            max_lr=max_lr,\n",
    "                            step_size=step_size,\n",
    "                            max_m=max_m,\n",
    "                            base_m=base_m,\n",
    "                            cyclical_momentum=cyclical_momentum)\n",
    "    \n",
    "    return [checkpoint_cb, reduceLR_cb, tensorboard_cb, early_stopping_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model_vanilla():\n",
    "    \n",
    "    input_ = Input(shape=(64,64,1))\n",
    "    conv1 = Conv2D(filters=6, kernel_size=(3, 3), activation='relu')(input_)\n",
    "    avgpool1 = AvgPool2D()(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(avgpool1)\n",
    "    avgpool2 = AvgPool2D()(conv2)\n",
    "\n",
    "    flat = Flatten()(avgpool2)\n",
    "    \n",
    "    dense_root_1 = Dense(2000,activation='relu')(flat)\n",
    "    dense_root_2 = Dense(1500,activation='relu')(dense_root_1)\n",
    "    dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "    dense_root_4 = Dense(800,activation='relu')(dense_root_3)\n",
    "    dense_root_5 = Dense(250,activation='relu')(dense_root_4)\n",
    "    output_root = Dense(168,activation='softmax',name='root')(dense_root_4)\n",
    "    \n",
    "    dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "    dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "    output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "    dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "    dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "    output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_vanilla = model_vanilla()\n",
    "keras.utils.plot_model(model_vanilla, '../results/vanilla_model.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_vanilla.compile(loss = keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1507 steps, validate for 503 steps\n",
      "Epoch 1/5\n",
      "1507/1507 [==============================] - 113s 75ms/step - loss: 1414.9798 - root_loss: 1392.5065 - vowel_loss: 8.4950 - consonant_loss: 13.9780 - root_accuracy: 0.0268 - vowel_accuracy: 0.1913 - consonant_accuracy: 0.6267 - val_loss: 8.0526 - val_root_loss: 4.7213 - val_vowel_loss: 2.1422 - val_consonant_loss: 1.1891 - val_root_accuracy: 0.0257 - val_vowel_accuracy: 0.1834 - val_consonant_accuracy: 0.6268\n",
      "Epoch 2/5\n",
      "1507/1507 [==============================] - 113s 75ms/step - loss: 8.0770 - root_loss: 4.7364 - vowel_loss: 2.1484 - consonant_loss: 1.1923 - root_accuracy: 0.0271 - vowel_accuracy: 0.1924 - consonant_accuracy: 0.6278 - val_loss: 8.0655 - val_root_loss: 4.7216 - val_vowel_loss: 2.1446 - val_consonant_loss: 1.1993 - val_root_accuracy: 0.0257 - val_vowel_accuracy: 0.1834 - val_consonant_accuracy: 0.6268\n",
      "Epoch 3/5\n",
      "1507/1507 [==============================] - 110s 73ms/step - loss: 8.0833 - root_loss: 4.7392 - vowel_loss: 2.1501 - consonant_loss: 1.1940 - root_accuracy: 0.0255 - vowel_accuracy: 0.1884 - consonant_accuracy: 0.6278 - val_loss: 8.0603 - val_root_loss: 4.7216 - val_vowel_loss: 2.1411 - val_consonant_loss: 1.1976 - val_root_accuracy: 0.0277 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n",
      "Epoch 4/5\n",
      "1507/1507 [==============================] - 107s 71ms/step - loss: 8.0809 - root_loss: 4.7371 - vowel_loss: 2.1504 - consonant_loss: 1.1934 - root_accuracy: 0.0264 - vowel_accuracy: 0.1925 - consonant_accuracy: 0.6278 - val_loss: 8.0535 - val_root_loss: 4.7235 - val_vowel_loss: 2.1396 - val_consonant_loss: 1.1904 - val_root_accuracy: 0.0291 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268s: 1.1926 - root_accuracy: 0.02\n",
      "Epoch 5/5\n",
      "1507/1507 [==============================] - 108s 72ms/step - loss: 8.0789 - root_loss: 4.7367 - vowel_loss: 2.1499 - consonant_loss: 1.1923 - root_accuracy: 0.0263 - vowel_accuracy: 0.1925 - consonant_accuracy: 0.6278 - val_loss: 8.0784 - val_root_loss: 4.7340 - val_vowel_loss: 2.1416 - val_consonant_loss: 1.2028 - val_root_accuracy: 0.0277 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n"
     ]
    }
   ],
   "source": [
    "history_vanilla = model_vanilla.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5,callbacks=get_callbacks('vanilla.h5',\n",
    "                                                                                 'vanilla.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla.save('../models/vanilla.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model_alexnet():\n",
    "    \n",
    "    input_ = Input(shape=(64,64,1))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', activation='relu')(input_)\n",
    "    maxpool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv1)\n",
    "    bn1 = BatchNormalization()(maxpool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu')(bn1)\n",
    "    maxpool2 = MaxPool2D(pool_size=(3, 3), strides=2, padding='valid')(conv2)\n",
    "    bn2 = BatchNormalization()(maxpool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(bn2)\n",
    "    drop1 = Dropout(0.5)(conv3)\n",
    "    conv4 = Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop1)\n",
    "    drop2 = Dropout(0.5)(conv4)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(drop2)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense_root_1 = Dense(4096,activation='relu')(flat)\n",
    "    dense_root_2 = Dense(4096,activation='relu')(dense_root_1)\n",
    "    dense_root_3 = Dense(1000,activation='relu')(dense_root_2)\n",
    "    output_root = Dense(168,activation='softmax',name='root')(dense_root_3)\n",
    "    \n",
    "    dense_vowel_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_vowel_2 = Dense(600,activation='relu')(dense_vowel_1)\n",
    "    dense_vowel_3 = Dense(100,activation='relu')(dense_vowel_2)\n",
    "    output_vowel = Dense(11,activation='softmax',name='vowel')(dense_vowel_3)\n",
    "    \n",
    "    dense_consonant_1 = Dense(800,activation='relu')(flat)\n",
    "    dense_consonant_2 = Dense(600,activation='relu')(dense_consonant_1)\n",
    "    dense_consonant_3 = Dense(100,activation='relu')(dense_consonant_2)\n",
    "    output_consonant = Dense(7,activation='softmax',name='consonant')(dense_consonant_3)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_],outputs=[output_root,output_vowel,output_consonant])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_alexnet = model_alexnet()\n",
    "keras.utils.plot_model(model_alexnet, '../results/alexnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_alexnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1507 steps, validate for 503 steps\n",
      "Epoch 1/5\n",
      "1507/1507 [==============================] - 341s 226ms/step - loss: 73.2590 - root_loss: 13.4151 - vowel_loss: 51.1728 - consonant_loss: 8.6710 - root_accuracy: 0.0278 - vowel_accuracy: 0.1915 - consonant_accuracy: 0.6262 - val_loss: 8.0602 - val_root_loss: 4.7263 - val_vowel_loss: 2.1456 - val_consonant_loss: 1.1884 - val_root_accuracy: 0.0291 - val_vowel_accuracy: 0.1834 - val_consonant_accuracy: 0.6268\n",
      "Epoch 2/5\n",
      "1507/1507 [==============================] - 314s 208ms/step - loss: 8.0791 - root_loss: 4.7378 - vowel_loss: 2.1490 - consonant_loss: 1.1923 - root_accuracy: 0.0270 - vowel_accuracy: 0.1938 - consonant_accuracy: 0.6278 - val_loss: 8.0637 - val_root_loss: 4.7253 - val_vowel_loss: 2.1466 - val_consonant_loss: 1.1918 - val_root_accuracy: 0.0203 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n",
      "Epoch 3/5\n",
      "1507/1507 [==============================] - 308s 204ms/step - loss: 8.4081 - root_loss: 4.7373 - vowel_loss: 2.4781 - consonant_loss: 1.1927 - root_accuracy: 0.0260 - vowel_accuracy: 0.1938 - consonant_accuracy: 0.6278 - val_loss: 8.0593 - val_root_loss: 4.7293 - val_vowel_loss: 2.1404 - val_consonant_loss: 1.1895 - val_root_accuracy: 0.0277 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n",
      "Epoch 4/5\n",
      "1507/1507 [==============================] - 315s 209ms/step - loss: 8.0796 - root_loss: 4.7376 - vowel_loss: 2.1483 - consonant_loss: 1.1936 - root_accuracy: 0.0278 - vowel_accuracy: 0.1909 - consonant_accuracy: 0.6278 - val_loss: 8.0757 - val_root_loss: 4.7300 - val_vowel_loss: 2.1545 - val_consonant_loss: 1.1911 - val_root_accuracy: 0.0219 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n",
      "Epoch 5/5\n",
      "1507/1507 [==============================] - 331s 220ms/step - loss: 8.0802 - root_loss: 4.7371 - vowel_loss: 2.1504 - consonant_loss: 1.1927 - root_accuracy: 0.0269 - vowel_accuracy: 0.1925 - consonant_accuracy: 0.6278 - val_loss: 8.0633 - val_root_loss: 4.7241 - val_vowel_loss: 2.1454 - val_consonant_loss: 1.1937 - val_root_accuracy: 0.0291 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6268\n"
     ]
    }
   ],
   "source": [
    "history_alexnet = model_alexnet.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5,callbacks=get_callbacks('alexnet.h5',\n",
    "                                                                                  'alexnet.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alexnet.save('../models/alexnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bengali_kaggle():\n",
    "    \n",
    "    input_ = Input(shape = (64, 64, 1))\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(137, 236, 1))(input_)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "#    model = BatchNormalization(momentum=0.15)(model)\n",
    "#    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "#    model = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "#    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(64, 64, 1))(input_)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation = \"relu\")(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    dense = Dense(512, activation = \"relu\")(model)\n",
    "    \n",
    "    head_root = Dense(168, activation = 'softmax',name='root')(dense)\n",
    "    head_vowel = Dense(11, activation = 'softmax',name='vowel')(dense)\n",
    "    head_consonant = Dense(7, activation = 'softmax',name='consonant')(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=input_, outputs=[head_root, head_vowel, head_consonant])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_kaggle = bengali_kaggle()\n",
    "keras.utils.plot_model(model_kaggle, '../results/bengali_kaggle.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_kaggle.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1507 steps, validate for 503 steps\n",
      "Epoch 1/5\n",
      "1507/1507 [==============================] - 1972s 1s/step - loss: 20.9063 - root_loss: 10.2477 - vowel_loss: 8.3481 - consonant_loss: 2.3106 - root_accuracy: 0.0272 - vowel_accuracy: 0.1879 - consonant_accuracy: 0.6256 - val_loss: 66657249.0145 - val_root_loss: 35271372.0000 - val_vowel_loss: 12374460.0000 - val_consonant_loss: 19011400.0000 - val_root_accuracy: 0.0273 - val_vowel_accuracy: 0.1779 - val_consonant_accuracy: 0.6268\n",
      "Epoch 2/5\n",
      "1507/1507 [==============================] - 2335s 2s/step - loss: 8.8598 - root_loss: 5.1219 - vowel_loss: 2.3304 - consonant_loss: 1.4075 - root_accuracy: 0.0257 - vowel_accuracy: 0.1927 - consonant_accuracy: 0.6274 - val_loss: 159626982.7464 - val_root_loss: 100653064.0000 - val_vowel_loss: 47369892.0000 - val_consonant_loss: 11604027.0000 - val_root_accuracy: 0.0273 - val_vowel_accuracy: 0.1986 - val_consonant_accuracy: 0.6268\n",
      "Epoch 3/5\n",
      "1507/1507 [==============================] - 2022s 1s/step - loss: 8.0800 - root_loss: 4.7382 - vowel_loss: 2.1494 - consonant_loss: 1.1924 - root_accuracy: 0.0242 - vowel_accuracy: 0.1911 - consonant_accuracy: 0.6278 - val_loss: 79336980720.1807 - val_root_loss: 44215037952.0000 - val_vowel_loss: 14875273216.0000 - val_consonant_loss: 20246659072.0000 - val_root_accuracy: 0.0289 - val_vowel_accuracy: 0.1946 - val_consonant_accuracy: 0.6268\n",
      "Epoch 4/5\n",
      "1507/1507 [==============================] - 1965s 1s/step - loss: 8.7874 - root_loss: 5.0732 - vowel_loss: 2.3885 - consonant_loss: 1.3257 - root_accuracy: 0.0272 - vowel_accuracy: 0.1903 - consonant_accuracy: 0.6277 - val_loss: 8588205241.7602 - val_root_loss: 5841961984.0000 - val_vowel_loss: 2286778624.0000 - val_consonant_loss: 459465376.0000 - val_root_accuracy: 0.0267 - val_vowel_accuracy: 0.1796 - val_consonant_accuracy: 0.6246\n",
      "Epoch 5/5\n",
      "1507/1507 [==============================] - 1961s 1s/step - loss: 9.0320 - root_loss: 5.2864 - vowel_loss: 2.2534 - consonant_loss: 1.4921 - root_accuracy: 0.0273 - vowel_accuracy: 0.1913 - consonant_accuracy: 0.6277 - val_loss: 2506914896.2532 - val_root_loss: 1550932864.0000 - val_vowel_loss: 955967552.0000 - val_consonant_loss: 14460.7959 - val_root_accuracy: 0.0257 - val_vowel_accuracy: 0.1998 - val_consonant_accuracy: 0.6264\n"
     ]
    }
   ],
   "source": [
    "history_kaggle = model_kaggle.fit(train_gen_image,\n",
    "                                    validation_data=valid_gen_image,\n",
    "                                    epochs = 5,callbacks=get_callbacks('bengali_kaggle.h5',\n",
    "                                                                       'bengali_kaggle.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kaggle.save('../models/bengali_kaggle.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_resnet = keras.applications.resnet_v2.ResNet152V2(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = keras.layers.GlobalAveragePooling2D()(base_model_resnet.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_resnet)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_resnet)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = keras.Model(inputs=base_model_resnet.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_resnet, '../results/resnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in model_resnet.layers:\n",
    "    print(str(layer)+ ' ' +str(layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_resnet = model_resnet.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('resnet50.h5','resnet50.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.save('../models/resnet50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_vggnet = keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet = keras.layers.GlobalAveragePooling2D()(base_model_vggnet.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_vggnet)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_vggnet)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_vggnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet = keras.Model(inputs=base_model_vggnet.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_vggnet, '../results/vggnet.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_vggnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vggnet.layers:\n",
    "    print(str(layer)+ ' ' +str(layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vggnet = model_vggnet.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('vggnet50.h5','vggnet50.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggnet.save('../models/vgg16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_xception = keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception = keras.layers.GlobalAveragePooling2D()(base_model_xception.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_xception)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_xception)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception = keras.Model(inputs=base_model_xception.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_xception, '../results/xception.png', expand_nested=True, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model_xception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_xception = model_xception.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('xception.h5','xception.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.save('../models/xception.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "base_model_inception = keras.applications.inception_v3.InceptionV3(weights='imagenet',\n",
    "                                                      include_top=False,\n",
    "                                                      input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = keras.layers.GlobalAveragePooling2D()(base_model_inception.output)\n",
    "head_root = (Dense(168, activation = 'softmax',name='root'))(model_inception)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel')(model_inception)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant')(model_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = keras.Model(inputs=base_model_inception.input,outputs=[head_root, head_vowel, head_consonant])\n",
    "keras.utils.plot_model(model_inception, '../results/inception.png', expand_nested=False, show_shapes=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15063 validated image filenames.\n",
      "Found 5021 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "model_inception.compile(loss = [keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy,\n",
    "                              keras.losses.sparse_categorical_crossentropy],\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.05),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(train_gen_image, valid_gen_image) = get_image_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_inception = model_inception.fit(train_gen_image,\n",
    "                                  validation_data=valid_gen_image,\n",
    "                                  epochs = 3,callbacks=get_callbacks('inception.h5','inception.h5','vowel_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception.save('../models/inception.h5')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
